<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>S.Lott -- Software Architect - Python</title><link href="https://slott56.github.io/" rel="alternate"></link><link href="/feeds/python.atom.xml" rel="self"></link><id>https://slott56.github.io/</id><updated>2008-08-13T10:10:00-04:00</updated><entry><title>Testing RESTful web services in Django -- Tantalizingly Close.</title><link href="https://slott56.github.io/2008_08_13-testing_restful_web_services_in_django_tantalizingly_close.html" rel="alternate"></link><published>2008-08-13T10:10:00-04:00</published><updated>2008-08-13T10:10:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2008-08-13:/2008_08_13-testing_restful_web_services_in_django_tantalizingly_close.html</id><summary type="html">&lt;p&gt;Here's what's great about &lt;a class="reference external" href="http://www.djangoproject.com"&gt;Django&lt;/a&gt;  coupled with the &lt;a class="reference external" href="http://code.google.com/p/django-rest-interface/"&gt;Django-REST Interface&lt;/a&gt; :  It's almost all model.  You define the model, write some tests.  Add the URL mappings, write some tests using the built-in Django client.&lt;/p&gt;
&lt;p&gt;We're almost there, but this doesn't work out perfectly.  To do complete tests, we have to either …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here's what's great about &lt;a class="reference external" href="http://www.djangoproject.com"&gt;Django&lt;/a&gt;  coupled with the &lt;a class="reference external" href="http://code.google.com/p/django-rest-interface/"&gt;Django-REST Interface&lt;/a&gt; :  It's almost all model.  You define the model, write some tests.  Add the URL mappings, write some tests using the built-in Django client.&lt;/p&gt;
&lt;p&gt;We're almost there, but this doesn't work out perfectly.  To do complete tests, we have to either subclass the Django Client to add &amp;quot;put&amp;quot; and &amp;quot;delete&amp;quot; or curry in methods for &amp;quot;put&amp;quot; and &amp;quot;delete&amp;quot;.  Then we can almost test our complete set of web services functions.&lt;/p&gt;
&lt;p&gt;At this point, the core of the application is -- well -- done.  It works, it handles the web services requests.  We can then start folding in HTML pages for the endlessly negotiated human interface.&lt;/p&gt;
&lt;p&gt;However, we're still not ready for deployment.&lt;/p&gt;
&lt;div class="section" id="authorization-differences"&gt;
&lt;h2&gt;Authorization Differences&lt;/h2&gt;
&lt;p&gt;First, we haven't really got a solid security model in place.  Sure, we can add &amp;#64;login_required decorators to any view functions.  But that doesn't really secure the REST interface at all.  That's where the going gets tough.&lt;/p&gt;
&lt;p&gt;The Django-REST Collection has an 'authentication' attribute that checks passwords.  It has an HttpDigestAuthentication class that handles more-secure password digests.  This looks perfect for web services.  But, it has two problems.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;We don't have MD5 digests readily available.  Django uses SHA1 digests of password only, not an MD5 digest of username:realm:password.&lt;/li&gt;
&lt;li&gt;We can't easily test using digest authentication with the off-the-shelf Django test Client.  Not only does the test client lack Put and Delete, but it can't handle HTTP Digest authentication, either.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Sigh.  I thought we'd be done in &lt;a class="reference external" href="http://showmedo.com/videos/video?name=2000080&amp;amp;fromSeriesID=200"&gt;20 minutes&lt;/a&gt; .  Turns out, I have to actually do some work.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="adding-md5-digests"&gt;
&lt;h2&gt;Adding MD5 Digests&lt;/h2&gt;
&lt;p&gt;MD5 digests seem to work out best with the 'Profile' extension to the Django authorization application.  The model is delightfully simple, just a single CharField to hold the MD5 hexdigest of username:realm:password.&lt;/p&gt;
&lt;p&gt;One consequence is that we now have two password digests, the default SHA1 in the User model and our Web Services MD5 in the Profile extension.  This means that our page for password resets must have a view that sets both passwords.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="testing-complications"&gt;
&lt;h2&gt;Testing Complications&lt;/h2&gt;
&lt;p&gt;In the long run, we have to provide WS client libraries.  While the application is entirely RESTful, the marketplace expects an API library that they can install.  We have to provide Python, .NET and Java libraries to invoke our service.  This isn't very complex.&lt;/p&gt;
&lt;p&gt;For Python, it would be simplest to leverage the &lt;a class="reference external" href="http://docs.python.org/lib/module-urllib2.html"&gt;urllib2&lt;/a&gt;  package.   We can provide some classes which act as remote procedure call proxies; these classes have methods that invoke our REST services (GET, POST, PUT and DELETE) on various resources or collections.&lt;/p&gt;
&lt;p&gt;Something like the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
class MyProxy( object ):
    def __init__( self, host, port, username, password, realm ):
        self.urlBase= &amp;quot;http://%s:%s&amp;quot; % ( host, port )
        # Build Handler to support HTTP Digest Authentication...
        digest_handler = urllib2.HTTPDigestAuthHandler()
        if username is not None:
            digest_handler.add_password(realm, self.urlBase, username, password)
        # Build Handler to support HTTP Basic Authentication...
        basic_handler = urllib2.HTTPBasicAuthHandler()
        if username is not None:
            basic_handler.add_password(realm, self.urlBase, username, password)
        # Build Handler to treat 201 as a normal response, not an exception...
        error_handler= RESTHTTPHandler()
        self.server = urllib2.build_opener(digest_handler,basic_handler,error_handler)
    def request( self, method, uri ):
        assert method in ( &amp;quot;GET&amp;quot;, &amp;quot;POST&amp;quot;, &amp;quot;PUT&amp;quot;, &amp;quot;DELETE&amp;quot; )
        data= urllib.urlencode( argDict )
        theReq= RESTRequest( method, self.urlBase + path, data )
        try:
            response= self.server.open( theReq )
            # fold in attributes that are compatible with Django HttpResponse
            response.status_code = response.code
            response.content= response.read()
            return response
        except:
            ... handle various kinds of IOError, HTTPError exceptions...
    def getSomeResource( self, key ):
        response= self.request( &amp;quot;GET&amp;quot;, &amp;quot;/path/to/resource/%s&amp;quot; % key )
        ... examine response.content, maybe do simplejson decode or xml.etree parse...
&lt;/pre&gt;
&lt;p&gt;The problem is that the Django test client and the urllib2 packages are wildly incompatible.&lt;/p&gt;
&lt;p&gt;Okay, maybe not &lt;em&gt;wildly&lt;/em&gt; , but seriously incompatible.&lt;/p&gt;
&lt;p&gt;First, the Django Client's HttpResopnse includes attributes status_code and content.  The urllib2.addinfourl response uses code and is -- itself -- a file-like object.&lt;/p&gt;
&lt;p&gt;Second, and more important, the Django Client's HttpResponse is a dictionary full of headers.  The urllib2.addinfourl is a file with an info() method that contains the headers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="choices"&gt;
&lt;h2&gt;Choices&lt;/h2&gt;
&lt;p&gt;We have a tantalizing set of alternatives.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Make urllib2's response look more like Django's response.  This requires adding a few additional attributes, and a __getitem__ method.  Not too difficult to do.  But only because our unit tests are not very demanding.&lt;/li&gt;
&lt;li&gt;Create a Facade over urllib2.addinfourl and django.http.HttpResponse that is independent of both, and can work with both as implementation classes.  While cool-sounding, and easy to implement in our WS client package, we'd have to do a tiny bit of extra work in our unit tests to create a Facade-based client rather than use the default client.&lt;/li&gt;
&lt;li&gt;Get a proper Python RESTful client.  Like &lt;a class="reference external" href="http://restclient.org/"&gt;RESTClient&lt;/a&gt;  or &lt;a class="reference external" href="http://code.google.com/p/python-rest-client/"&gt;Python-rest-client&lt;/a&gt; .  The approach in &lt;a class="reference external" href="http://www.infectmac.com/2008/08/restful-python.html"&gt;RESTful Python&lt;/a&gt;  -- a decorator -- is another possibility.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The problem with #1 is that the Python client package we distribute will have this odd-looking design that adds a bunch of random-looking features to urllib2.addinfourl.  A lot of explanation (like this Blog posting) doesn't remove the oddness.  The Java and .Net packages will be fine.&lt;/p&gt;
&lt;p&gt;The problem with #2 is that the Python client package will be even more complex than #1, with little recognizable value to anyone for the complexity.&lt;/p&gt;
&lt;p&gt;There's no problem with #3.  Indeed, this might be best in the long run.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>The Schema Evolution Problem</title><link href="https://slott56.github.io/2008_08_06-the_schema_evolution_problem.html" rel="alternate"></link><published>2008-08-06T10:21:00-04:00</published><updated>2008-08-06T10:21:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2008-08-06:/2008_08_06-the_schema_evolution_problem.html</id><summary type="html">&lt;p&gt;Fundamentally, we need to provide explicit version identification on a schema.   This is technically easy, but organizationally nearly impossible.&lt;/p&gt;
&lt;p&gt;Technically, we need to use some kind of version control software for our model and the resulting DDL.  We need some meta-meta-data to track schema names and version numbers.  If we …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Fundamentally, we need to provide explicit version identification on a schema.   This is technically easy, but organizationally nearly impossible.&lt;/p&gt;
&lt;p&gt;Technically, we need to use some kind of version control software for our model and the resulting DDL.  We need some meta-meta-data to track schema names and version numbers.  If we like doing too much work, we can introduce a meta-meta-data table with schema name and version numbers.  If we're lazy, there's an even simpler, more reliable approach.&lt;/p&gt;
&lt;p&gt;Organizationally, we need the discipline to track every single schema change and determine the level of compatibility with application software.  Garden-variety ALTER statements (to add columns, or extend the size of a column) won't break software; bumping the minor version number is fine.  Adding new tables or views won't break software.  Renames and drops, however, will break software and require a bump to the major version number.&lt;/p&gt;
&lt;div class="section" id="what-is-a-schema"&gt;
&lt;h2&gt;What is a Schema?&lt;/h2&gt;
&lt;p&gt;First, a schema isn't the &lt;em&gt;entire&lt;/em&gt;  set of metadata in a single database instance.  Even if your data is organized in one massive, flat schema with thousands of tables, you still have many smaller &amp;quot;schema&amp;quot; within that single SQL schema owned by &amp;quot;PROD&amp;quot; or &amp;quot;OPS&amp;quot; or &amp;quot;DBA&amp;quot; or &amp;quot;SYS&amp;quot; or whoever owns your production tables.&lt;/p&gt;
&lt;p&gt;We'll distinguish between the practical, conceptual schema and the often-misused SQL schema.  Sometimes they overlap, but this is rare.&lt;/p&gt;
&lt;p&gt;Your smaller conceptual schemas are the &amp;quot;application-specific&amp;quot; subsets of your overall SQL schema.  If you're smart, your SQL schemas match your conceptual schemas.  If you're lazy, you have a single massive SQL schema and use table name prefixes to try and separate tables into smaller conceptual schemas.&lt;/p&gt;
&lt;p&gt;Here's the bottom-line suggestion. Use SQL schema.  Don't use table prefixes.&lt;/p&gt;
&lt;p&gt;[In a Big IT organization, this can't happen because it would &amp;quot;break everything&amp;quot;.  Everyone depends on there being a single, flat anonymous schema.  This isn't true, as new applications and maintenance to existing applications are an opportunity to restructure the SQL schema to match the actual use of the tables.  Sadly, it only reduces future maintenance costs, so it doesn't have any current-year impact, so no one ever does this.]&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-about-the-applications"&gt;
&lt;h2&gt;What About the Applications?&lt;/h2&gt;
&lt;p&gt;The applications exist independent of the data.  Stored procedures (&lt;a class="reference external" href="https://slott56.github.io/2008_08_03-stored_procedures_are_a_configuration_management_nightmare_revised.html"&gt;A Configuration Management Nightmare&lt;/a&gt; ) are in the application model, not the data model, and evolve independently from the data schema.  However, this isn't always understood, and stored procedures are often mis-managed.&lt;/p&gt;
&lt;p&gt;An application could check the schema meta-meta-data to be sure that the application is compatible with the schemas it uses.  It can be a simple query, and an exception gets thrown to indicate that the application can't start and run with the given mix of database schemas.  We know that production programs shouldn't work with the new, upgraded integration test database.  However, we also see this happen; sometimes they crash and we fix them, other times they don't crash, but don't produce right answers, either.  Sigh.&lt;/p&gt;
&lt;p&gt;There's a simpler approach, however, than a query.&lt;/p&gt;
&lt;p&gt;Should the application and schema version numbers track?  Should the application go through version 2.1.2 and 2.1.3 to indicate that it requires schema version 2.1?  Not necessarily.  There is not a tidy 1:1 mapping between software components and database schema objects.  Generally, database schema objects are shared -- widely -- by software components.  Version 2.1 of application X and version 4.2 of application Y may both depend on version 3.x of database schema Z.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-to-make-this-work"&gt;
&lt;h2&gt;How To Make This Work&lt;/h2&gt;
&lt;p&gt;Put the version number in the schema name.&lt;/p&gt;
&lt;p&gt;First, don't create a bunch of XYZ_table1, XYZ_table2 names in a single, flat schema.  Create table1 and table2 in schema XYZ.  Use lots of schemas.  That's why they're available to you.&lt;/p&gt;
&lt;p&gt;[Yes, your historical, legacy applications didn't use schemas.  I'm aware that this is new.  Start now.]&lt;/p&gt;
&lt;p&gt;Second, don't simply create a &amp;quot;timeless&amp;quot; XYZ schema, use the major release number as part of the name.  Create an XYZ_2 schema.  This will work for all 2.x versions of the schema.&lt;/p&gt;
&lt;p&gt;When you move to version 3.1, create a new XYZ_3 schema.  &lt;strong&gt;New&lt;/strong&gt;.  Migrate the data from the XYZ_2 schema.  Then, rename XYZ_2 to XYZ_2_OLD, so that any program that improperly uses the old schema will throw an exception and die.  When you need to recover the space, you can drop the XYZ_2_OLD schema, knowing that no program is expected to use it; any program that does use it, needs a fix.&lt;/p&gt;
&lt;p&gt;Wait!  That's potentially a lot of code to touch.  Or, if your a mainframer, that's a lot of programs that need to be rebound to the new SQL.  Yep.  It is.  It's a trivial administrative task.  If you can't recompile or rebind your programs, you have serious quality issues that you &lt;strong&gt;must&lt;/strong&gt;  fix.&lt;/p&gt;
&lt;p&gt;If you can't make simple SQL changes, you have serious flaws in your application software and your overall IT processes.  You &lt;strong&gt;must&lt;/strong&gt;  fix these application design flaws and organizational process flaws.  I'm sorry for pointing this out.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="implementation-steps"&gt;
&lt;h2&gt;Implementation Steps&lt;/h2&gt;
&lt;p&gt;Name your schema.  Allocate your tables to appropriate schema.  More schemas is a better approach than fewer schemas.  There's no performance penalty.  Design and maintenance are simpler.  When you design software, You don't design a single, massive, does-everything application, you write small, focused application programs.  Your database, similarly, should be structured in small, conceptually simple modules.&lt;/p&gt;
&lt;p&gt;For Java programmers, use &lt;a class="reference external" href="http://ibatis.apache.org/"&gt;iBatis&lt;/a&gt;  to extract your SQL from your programs.  The schema changes will be isolated to the iBatis configuration files, mostly.&lt;/p&gt;
&lt;p&gt;For Python programmers, you can use &lt;a class="reference external" href="http://www.sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt;  to isolate most of the SQL from your overall application.  Put each schema definition in a separate &amp;quot;models&amp;quot; file.  Include the SQLAlchemy table definitions as well as the Python classes and the mappings.  You can, without too much difficulty, include a few convenience functions that will create or drop-and-create the schema.&lt;/p&gt;
&lt;p&gt;If you're creating Python/Django applications, consider including the schema version number on your Django application name.  Your Django project folder for a given site might include things like someapp_1 and someapp_2.  The older application (someapp_1) has one model, and the newer version (someapp_2) has the expanded, incompatible model.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="change-management"&gt;
&lt;h2&gt;Change Management&lt;/h2&gt;
&lt;p&gt;Rather than mess with an complex, risky in-place conversion, you are &lt;em&gt;adding&lt;/em&gt;  to the database.  You can write a simple batch application to create the someapp_2 data objects from the someapp_1 objects.  Once the data is migrated, you can switch the settings.py and the urls.py files to use someapp_2 instead of someapp_1.  You can easily dry-run this conversion process in an integration test or staging instance of your web site.  If it works there, you can do it again in production.&lt;/p&gt;
&lt;p&gt;The best part about keeping the two schema in parallel for a time is the ability to fall-back to the previous version and try the conversion again after fixing the bugs.  You're never replacing anything; you're simply adding a schema and directing the application programs at the new schema.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>Stored Procedures Are A Configuration Management Nightmare (revised)</title><link href="https://slott56.github.io/2008_08_03-stored_procedures_are_a_configuration_management_nightmare_revised.html" rel="alternate"></link><published>2008-08-03T16:44:00-04:00</published><updated>2008-08-03T16:44:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2008-08-03:/2008_08_03-stored_procedures_are_a_configuration_management_nightmare_revised.html</id><summary type="html">&lt;p&gt;I've been asked about the proper location of Stored Procedures more than once.  I've come down very strongly in opposition to triggers and stored procedures.&lt;/p&gt;
&lt;p&gt;First, &lt;a class="reference external" href="https://slott56.github.io/2007_05_27-plsql_and_java_the_benchmark_challenge_revised.html"&gt;PL/SQL is slow&lt;/a&gt; .  Anecdotally, people claim that introducing PL/SQL made an app faster.  I submit that they restructured the application significantly to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been asked about the proper location of Stored Procedures more than once.  I've come down very strongly in opposition to triggers and stored procedures.&lt;/p&gt;
&lt;p&gt;First, &lt;a class="reference external" href="https://slott56.github.io/2007_05_27-plsql_and_java_the_benchmark_challenge_revised.html"&gt;PL/SQL is slow&lt;/a&gt; .  Anecdotally, people claim that introducing PL/SQL made an app faster.  I submit that they restructured the application significantly to create small, focused transactions, and that's what created the improvement.  As a practical matter, you need to write focused, PL/SQL-like transaction methods in your Java programs.  While technically possible, you can't casually execute SQL statements willy-nilly.&lt;/p&gt;
&lt;p&gt;Second, it's hard to do configuration management on stored procedures.  Not impossible, but very hard.  The reasons are entirely organizational.&lt;/p&gt;
&lt;p&gt;Recently I received an email that was nearly opaque, but seemed to indicate that the organization couldn't clone production to create another test, and couldn't rationalize the versions of their various stored procedures.   I think they wanted a puff of &lt;strong&gt;Faerie Dust&lt;/strong&gt;™ that would allow stored procedure X to determine if it was being used by package Y or package Z and behave differently in the different contexts.  The request makes no sense -- this is just a version control issue.  Clearly, there are two versions of X, but the emailer claimed there was one version of X and it had to determine it's behavior dynamically.&lt;/p&gt;
&lt;div class="section" id="conflation-the-organizational-root-cause"&gt;
&lt;h2&gt;Conflation - The Organizational Root Cause&lt;/h2&gt;
&lt;p&gt;A stored procedure lives in the database.  Consequently, it's conflated with persistent data and schema definitions and assigned -- for no good reason -- to the DBA's.  These three things -- data, schema and processing -- have little to do with each other.  They emphatically do not belong together.   However, they're almost always conflated into a murky puddle of SQL.&lt;/p&gt;
&lt;p&gt;Let's break these things apart.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;&lt;strong&gt;Data&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;is the organization's actual data.  Some (but not all) of the business records lives in managed databases.  Some live in desktop application documents (word processing, spreadsheets, unmanaged desktop databases, etc.)  Data is precious, perhaps the most precious thing in the organization.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Schema&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;(or metadata) is table, column, view and index definitions.  It's also physical stuff like tablespaces, files, instances, etc.  Some of this is important, some of it is subject to change without notice.  Tablespace configuration parameters rarely matter except as an implementation detail.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Processing&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;is triggers, stored procedures and all of the application programs that live outside the database.  Note that there is no crisp distinction between &amp;quot;low-level&amp;quot; and &amp;quot;high-level&amp;quot; processing.  Many DBA's have tried to explain to me that CRUD rules are &amp;quot;low-level&amp;quot;, but then they add some foreign-key relationships, after that they also need to add some many-to-many relationships and the intermediate bridge tables, then they start adding other things that are part of larger and more complex relationships.  Stop!  If you can't find a boundary easily, it doesn't really exist.&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class="section" id="version-control"&gt;
&lt;h2&gt;Version Control&lt;/h2&gt;
&lt;p&gt;Data -- typically -- has fairly loose version control.  The RDBMS often has secret sequence numbers (SCN's) that are used internally to manage cache and synchronize physical files.  These transaction sequence numbers are, effectively, a kind of version number for the data.&lt;/p&gt;
&lt;p&gt;Often, we'll have a &amp;quot;last changed date&amp;quot; in a database record.  This is a surrogate version number for the record.  It tells us when the data changed.  Most applications don't record a complete change log for the data, we simply update the change date.  A few applications do create detailed change logs.  In some cases, people try to leverage the database logging facilities to back into a formal change log for the data.&lt;/p&gt;
&lt;p&gt;Schema is rarely under any kind of version control.  Metadata is often the least disciplined part of the enterprise infrastructure.  It's easy (really easy) to have formal version control over metadata.  It's rarely done, however.  For some reason, DBA's don't seem to use version control software.&lt;/p&gt;
&lt;p&gt;Application software, typically, has the best version control.  Many organizations use some formal version control software (CVS, Subversion or some commercial product like MKS, VSS or PVCS.)  This can easily apply version control information to the source code (and even the resulting .class files.)&lt;/p&gt;
&lt;p&gt;[Some organizations can't even put their application software under version control.  This doesn't change the issue of conflating data, schema and application.]&lt;/p&gt;
&lt;p&gt;For no good reason stored procedures are the province of the DBA's (who don't use version control software.)  Consequently, the external application software (in Java, Python or whatever) may have version control information, but the stored procedures never have version control.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="schema-versions"&gt;
&lt;h2&gt;Schema Versions&lt;/h2&gt;
&lt;p&gt;The database schema (the tables, columns, indexes, views and sequence generators) has a version number.  The version number for a schema -- like the version number for software -- defines &amp;quot;compatibility&amp;quot;.&lt;/p&gt;
&lt;p&gt;Schema version 2.1 and 2.2 are &amp;quot;compatible&amp;quot; in some sense.  Schema versions 3.5 and 4.1 are incompatible.&lt;/p&gt;
&lt;p&gt;What defines &amp;quot;compatibility&amp;quot;?  Clearly, &amp;quot;compatible&amp;quot; means &amp;quot;compatible with SQL DML&amp;quot;.  If you've done standard database ALTER statements (adding columns, expanding the sizes of columns) or changing indexes or adding views, you haven't broken any SQL DML.  The old SQL still works with the new schema.  This is a 2.2 to 2.3 kind of change.&lt;/p&gt;
&lt;p&gt;If you've dropped a column or table or view, or you've shortened a column, or changed the type of a column, then you've made a change which may break existing SQL.  When you've make this kind of change, you'll need to bump the major version number.&lt;/p&gt;
&lt;p&gt;You need two things:&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;&lt;strong&gt;Discipline&lt;/strong&gt;.&lt;/dt&gt;
&lt;dd&gt;This doesn't happen by default.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Some meta-meta-data&lt;/strong&gt;.&lt;/dt&gt;
&lt;dd&gt;A table that has schema names and version numbers is all you really need.  It's nice to fold in &amp;quot;applicable dates&amp;quot; and &amp;quot;responsible person&amp;quot;, etc., but not essential.   In some cases, you can use database comments for this.&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;When you make database changes, you must create a script that (a) makes the change and (b) updates the database schema version table.  That's about it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="what-about-stored-procedures"&gt;
&lt;h2&gt;What About Stored Procedures?&lt;/h2&gt;
&lt;p&gt;Why can't we annotate our stored procedures with version numbers and put them under version control like the rest of the database?&lt;/p&gt;
&lt;p&gt;The question is rhetorical.  Of course we can put stored procedures under version control.  It just requires some discipline.  And -- perhaps -- making stored procedures part of application software's responsibility, and not part of the DBA's job.&lt;/p&gt;
&lt;p&gt;If we take stored procedures away from the DBA's, we need a formal turnover procedure for putting a particular suite of stored procedures into a database.&lt;/p&gt;
&lt;p&gt;Separating the stored procedures from the schema via a formal turnover has some marvelous consequences.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;You can reconstruct the stored procedures from your source code repository exactly the same way you extract your Python or Java.  Indeed, you can make a complete software package with all of the various language elements.  You can extract all of the procedure creates as a big script and run it any time you need to.&lt;/li&gt;
&lt;li&gt;The database has two distinct parts:  the Data, the Processing.  These two are matched by schema version number.  The DBA's are responsible for the data; the schema versions; the preservation of essential corporate information.  The DBA's are also responsible for running the scripts that upgrade that portion of the application software that happens to live in the database.  The DBA's aren't responsible for stored procedures.&lt;/li&gt;
&lt;li&gt;The migration of a database from development to test is a two-part job.  Move the schema and data from the developers to a test environment.  Separately, run all of the scripts to build the proper software version that matches the schema of the data.&lt;/li&gt;
&lt;li&gt;You have explicit compatibility checks.  Version 2.x of schema and software are being used in production.  Version 3.x of schema and software is in some kind of parallel test prior to conversion.  Version 3.y of schema and software is in some early test; 3.z is in development.&lt;/li&gt;
&lt;li&gt;You can begin to wean yourself away from the nightmare of stored procedure management.  Once you take this out of the DBA's hands, you find that a consistent set of Python (or Java) packages that define the Model layer does everything that stored procedures and triggers do, only more simply and more maintainably.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class="section" id="what-s-so-hard"&gt;
&lt;h2&gt;What's So Hard?&lt;/h2&gt;
&lt;p&gt;It's very easy to put stored procedures under explicit, clear version control.  With a little care, even a database schema can be put under version control.&lt;/p&gt;
&lt;p&gt;What's so hard is actually making the organizational change.  Ask around.  The DBA's will tell you that they are overworked, because they're &amp;quot;forced&amp;quot; to write all the stored procedures and triggers.  Forced?  By whom?&lt;/p&gt;
&lt;p&gt;Generally, the &amp;quot;organization&amp;quot; seems to mandate that everything SQL -- tables, columns, indexes, views, stored procedures and triggers -- pass through the DBA's.  The distinction between data and processing is somehow lost.  Splitting it up will often anger the manager of the DBA's, who'll make the case that no one else can be trusted to create stored procedures.&lt;/p&gt;
&lt;p&gt;When testing stops because of version control issues, when production fails, it seems like the problem should be addressed.  It's usually obvious that there are serious version control problems between the schema and stored procedures.&lt;/p&gt;
&lt;p&gt;I only know that there's a long-standing, steadfast refusal to split the database into data and processing elements.  The consequence of this is that stored procedures are unmaintainable, testing is nearly impossible, and production problems are rampant.&lt;/p&gt;
&lt;p&gt;Consequently, I suggest that stored procedures and triggers never be used.  Ever.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>Denormalization or "What did you mean by that?"</title><link href="https://slott56.github.io/2008_06_14-denormalization_or_what_did_you_mean_by_that.html" rel="alternate"></link><published>2008-06-14T11:59:00-04:00</published><updated>2008-06-14T11:59:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2008-06-14:/2008_06_14-denormalization_or_what_did_you_mean_by_that.html</id><summary type="html">&lt;p&gt;I use the word denormalization heavily, to make a point to a certain class of developers.  Other developers object to the term, since it doesn't have a precise meaning.&lt;/p&gt;
&lt;p&gt;The point I often have to make this:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;3rd Normal Form is for Updates.&lt;/li&gt;
&lt;li&gt;Data Warehousing is about Insert and Select …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;I use the word denormalization heavily, to make a point to a certain class of developers.  Other developers object to the term, since it doesn't have a precise meaning.&lt;/p&gt;
&lt;p&gt;The point I often have to make this:&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;3rd Normal Form is for Updates.&lt;/li&gt;
&lt;li&gt;Data Warehousing is about Insert and Select; there are no Updates (to speak of).&lt;/li&gt;
&lt;li&gt;Consequently, the traditional normalization rules (Third Normal Form a/k/a 3NF) doesn't apply to data warehousing.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My habit is to describe the star-schema (or snowflake schema) as &amp;quot;denormalized&amp;quot;.  This isn't really correct, but it does emphasize my point.  I have to make this point emphatically because we have to get past the Data Cartel's Standard Objection: &lt;strong&gt;New Technology Won't Work&lt;/strong&gt;.  Most DBA's who are new to Data Warehousing and the star schema will exercise their veto authority over new technology, claim that the design is &amp;quot;inefficient&amp;quot; and stop (or delay) the project.&lt;/p&gt;
&lt;div class="section" id="dba-objections"&gt;
&lt;h2&gt;DBA Objections&lt;/h2&gt;
&lt;p&gt;DBA's can object in &lt;a class="reference external" href="https://slott56.github.io/2007_11_29-the_passive_aggressive_programmer_or_why_nothing_gets_done_revised.html"&gt;Passive-Aggressive&lt;/a&gt;  (and &lt;a class="reference external" href="https://slott56.github.io/2008_03_24-the_passive_aggressive_programmer_part_ii.html"&gt;Passive-Aggressive Part II&lt;/a&gt; ) mode -- where they don't have a better solution, they just have &amp;quot;concerns&amp;quot; about the standard DW solution.  Here are some things I've heard.&lt;/p&gt;
&lt;ol class="arabic"&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;strong&gt;It isn't normalized&lt;/strong&gt;.  Which is a WTF? kind of point.  It isn't normalized for updates because there aren't any (to speak of).  It's normalized for SELECT SUM(*) GROUP BY, which is the canonical dimensional query.  I call this &amp;quot;denormalization&amp;quot; to make the point; perhaps I should call it star-schema normalization.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;strong&gt;It doesn't use &amp;quot;natural keys&amp;quot; correctly&lt;/strong&gt;.  I'm pretty sure that natural keys don't actually exist.  Almost everything is either an attribute (which can change) or a surrogate key (which isn't very likely to change).  A changeable attribute isn't really a key, is it?&lt;/p&gt;
&lt;p&gt;When writing ETL programs, we sometimes have a blurry edge when an external application assigns a truly permanent surrogate key.  In these cases, the external surrogate is often something that the organization uses heavily -- as if it was a natural key.  In other cases, they have a surrogate-like key that can (it turns out) change, making it just an attribute.  In the warehouse, it's usually best to simply assign warehouse surrogates and not burn up brain calories trying to make too many distinctions in the source applications.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;strong&gt;All those joins are inefficient&lt;/strong&gt;.  This can -- in the extreme case -- lead to &lt;strong&gt;The Uni-Table&lt;/strong&gt;.  This is the pre-joined ur-fact table that contains all dimensional attributes and all fact values.  It works, but it repeats all of the dimensional attributes and it doesn't track dimensional change at all.  Yes, I've seen it done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;strong&gt;It uses too much storage&lt;/strong&gt;.  This is just silly, but it comes up.  Once, I caught the sysadmins and DBA's in a meeting where they were quibbling about log sizes so that they could micro-manage storage at the 100Gb increment.  &amp;quot;There's four people in this meeting.  At your hourly cost, I could have bought 400Gb at Circuit City.&amp;quot;  And the price of storage continues to plummet.  Nowadays, I think I could buy a terabyte.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;strong&gt;Fact updates can be inefficient&lt;/strong&gt;.  This is crazy, because changing a fact's measurement value is a single row update; it's fine if you're correcting errors.  Changing a batch of fact's measurements is -- what? -- criminal mischief?  Who changes batches of facts?  Considerer deleting the incorrect ones and reloading correct ones.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Changing the association between a batch of facts and a dimension is even spookier.  The historical fact is what you recorded.  You don't get to change the facts; it's called perjury.  If you're restating your books, you usually have new facts that apply to a historical time period.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="star-schema-normalization"&gt;
&lt;h2&gt;Star Schema Normalization&lt;/h2&gt;
&lt;p&gt;To get past the DBA objections, we need to have several heart-to-heart conversations on star-schema normalization.  Generally, these are painful because DBA's are overworked and sometimes underqualified.  Examples on paper don't help much.  Telling them to read Kimball does nothing.  Loading up realistic sets of data is the only workable approach to showing them that the storage is manageable, the joins won't kill you, surrogate keys work, and the star schema is a &amp;quot;real&amp;quot; thing.&lt;/p&gt;
&lt;p&gt;Once we've aired out an example, we then have to revisit the star schema thing over and over again.  Most DBA's are so habituated to 3NF that they can't get past it to see that a star schema is an alternative normal form.  Except in rare cases, the best we get is grudging tolerance.  [In the rare cases where the DBA's embrace a star schema approach, no one needs me, except to validate the design.]&lt;/p&gt;
&lt;p&gt;The basic 1NF and 2NF rules apply to the star schema normal form as well as transactional normal form.  Arrays are still a bad idea in the relational world.  Foreign attributes (those not functionally dependent on the key) are still a bad idea.  However, 3NF is out the window -- derived data is a helpful thing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="derived-data-what-about-updates"&gt;
&lt;h2&gt;Derived Data?  What About Updates?&lt;/h2&gt;
&lt;p&gt;DBA standard objection #5 -- updates hurt -- often surfaces when discussing the approach of persisting derived data.  This is a focused &amp;quot;denormalization&amp;quot; that unwinds just 3NF to avoid repeating a calculation.  In the case of a data warehouse (load once, query an infinite number of times) all calculations done at load time are amortized across an infinite number of queries, making them delightfully efficient.&lt;/p&gt;
&lt;p&gt;The &amp;quot;update&amp;quot; issue can't arise.  Let's look at some common dimensions.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;&lt;strong&gt;Time&lt;/strong&gt;.&lt;/dt&gt;
&lt;dd&gt;You don't change the day of the week for March 8, 1987.  It is, was, and always will be Sunday.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Space&lt;/strong&gt;.&lt;/dt&gt;
&lt;dd&gt;Geographical boundaries change.  However, this is the canonical Slowly Changing Dimension (SCD) problem that Kimball covers in detail.  [If you have what Kimball calls a &amp;quot;type 3&amp;quot; SCD, you have the most common example of an update in a data warehouse; the change of status from &amp;quot;current&amp;quot; to &amp;quot;previous&amp;quot;.]&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Customer&lt;/strong&gt;.&lt;/dt&gt;
&lt;dd&gt;Your customers (either individuals in huge collections or other businesses in small collections) have numerous changes.  However, they often have attributes which can't change as well as attributes which frequently change.  For example, demographics change very slowly (if at all).  Customers often requires more sophisticated &amp;quot;snowflake schema&amp;quot; techniques.  There still aren't any updates, but there are SCD techniques for handling this.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Product&lt;/strong&gt;.&lt;/dt&gt;
&lt;dd&gt;Your products, product lines, product families, product groupings, solutions, technologies, platforms, services, etc., are all grouped by marketing in the randomest ways.  These groupings and hierarchies and clusters and affinities are just ways that marketing tries to portray your company; and it changes with every whim and brain-fart.  This is also a basic SCD issue; you simply add the alternative hierarchies and groupings and do alternate joins on the facts.&lt;/dd&gt;
&lt;dt&gt;&lt;strong&gt;Cost Centers&lt;/strong&gt;.&lt;/dt&gt;
&lt;dd&gt;Your internal cost structure changes.  Sometimes frequently.  This is still SCD.  No updates, just inserts.&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div class="section" id="recent-example"&gt;
&lt;h2&gt;Recent Example&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://slott56.github.io/2008_06_06-my_query_is_slow_what_to_do_or_dumb_as_a_post_sql_revised.html"&gt;Recently&lt;/a&gt; , I aired out some brain-dead non-solutions to a simple reporting problem where number of rows (500 new rows per hour) might have been the design problem being solved.  The non-solution involved a five ways of avoiding a viable solution.  As follow-up to my suggestions, I was given the following variations on DBA objection 1; each affixing blame somewhere else.&lt;/p&gt;
&lt;p&gt;1.1.  The customer cannot accept a &amp;quot;denormalized&amp;quot; table that pre-computes the values.  [The customer is at fault.]&lt;/p&gt;
&lt;p&gt;1.2.  Since we can't directly use the &amp;quot;denormalized&amp;quot; table in your blog posting, the idea of denormalization is broken, and we can never talk about it in any form whatsoever.  Temporary tables, materialized views and other techniques are off the table, &lt;em&gt;a priori&lt;/em&gt;.  [I'm at fault for not providing the expected solution, which involved some kind of &lt;strong&gt;Faerie Dust&lt;/strong&gt;™ that would make a bad table process quickly.]&lt;/p&gt;
&lt;p&gt;1.3.  The organization can't learn anything new.  Talking about &amp;quot;denormalization&amp;quot; would be new, and is therefore forbidden.  The idea of persistent derived values is off the table, &lt;em&gt;a priori&lt;/em&gt;.  [The organization is at fault.]&lt;/p&gt;
&lt;p&gt;At this point, any suggestion I might have has been trumped by the DBA's opposition to denormalization.  Blame has been assigned everywhere.  I think this is because I used the word &amp;quot;denormalization&amp;quot; incautiously and set myself up for three flavors of DBA objection #1 (&amp;quot;It isn't normalized.&amp;quot;)&lt;/p&gt;
&lt;p&gt;Perhaps, if I'd said &amp;quot;persistent derived values&amp;quot; instead of &amp;quot;denormalization&amp;quot; we might have gotten somewhere.  Ideally, they would have suggested a temporary table or materialized view as an implementation technique.  But, we stalled out at my incautious use of a loaded buzzword.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>Genius Move -- Characteristic Functions</title><link href="https://slott56.github.io/2008_06_07-genius_move_characteristic_functions.html" rel="alternate"></link><published>2008-06-07T13:54:00-04:00</published><updated>2008-06-07T13:54:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2008-06-07:/2008_06_07-genius_move_characteristic_functions.html</id><summary type="html">&lt;p&gt;The comment was eaten by Haloscan, but here's the text...&lt;/p&gt;
&lt;p&gt;You need to read Rozhenstein on characteristic functions.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
select
sum(case when a &amp;lt; .5 then 1 else 0 end) 'A'
,sum(case when a &amp;gt;= .5 and a &amp;lt; .75 then 1 else 0 end) 'B'
,sum(case when a &amp;gt;= .75 then …&lt;/pre&gt;</summary><content type="html">&lt;p&gt;The comment was eaten by Haloscan, but here's the text...&lt;/p&gt;
&lt;p&gt;You need to read Rozhenstein on characteristic functions.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
select
sum(case when a &amp;lt; .5 then 1 else 0 end) 'A'
,sum(case when a &amp;gt;= .5 and a &amp;lt; .75 then 1 else 0 end) 'B'
,sum(case when a &amp;gt;= .75 then 1 else 0 end) 'C'
,bar
from foo
group by bar
&lt;/pre&gt;
&lt;p&gt;So, I googled it, and figured out what I'd been missing.&lt;/p&gt;
&lt;div class="section" id="skip-the-math"&gt;
&lt;h2&gt;Skip the Math&lt;/h2&gt;
&lt;p&gt;The Google page on characteristic functions is heavy going.  The issue here is to characterize the frequency distribution of some more-or-less random variable.  This is a real close fit with the formal definition of a characteristic function.&lt;/p&gt;
&lt;p&gt;When should we apply the characteristic function?  Load time or query time?  The comment showed it at query time.  However, we could also do it at load time.&lt;/p&gt;
&lt;p&gt;Here's the genius part.&lt;/p&gt;
&lt;p&gt;If we define it as a separate function, we can defer this decision based on which implementation meets our performance guidelines.&lt;/p&gt;
&lt;p&gt;We have this situation.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def c1( value ):
    a,b = divmod( int(value*100), 10 )
    if b == 0:
        return &amp;quot;== 0.%d&amp;quot; % ( a, )
    else:
        return &amp;quot;0.%d - 0.%d&amp;quot; % ( a, a+1 )
&lt;/pre&gt;
&lt;p&gt;We can then use this during load or we can use it in a fetch loop.  Quite cool.  Very elegantly separated from other parts of the processing.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>My Query Is Slow -- What To Do? Or Dumb-As-A-Post SQL (Revised)</title><link href="https://slott56.github.io/2008_06_06-my_query_is_slow_what_to_do_or_dumb_as_a_post_sql_revised.html" rel="alternate"></link><published>2008-06-06T22:30:00-04:00</published><updated>2008-06-06T22:30:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2008-06-06:/2008_06_06-my_query_is_slow_what_to_do_or_dumb_as_a_post_sql_revised.html</id><summary type="html">&lt;p&gt;First, let me point out that the Data Cartel (&amp;quot;DBA&amp;quot; means Don't Bother Asking) won't release all the information I requested, so some of this is a guess.&lt;/p&gt;
&lt;p&gt;We'll look at a number of dumb-as-a-post SQL techniques.  This is proof -- if any were needed -- that bad SQL is worse than …&lt;/p&gt;</summary><content type="html">&lt;p&gt;First, let me point out that the Data Cartel (&amp;quot;DBA&amp;quot; means Don't Bother Asking) won't release all the information I requested, so some of this is a guess.&lt;/p&gt;
&lt;p&gt;We'll look at a number of dumb-as-a-post SQL techniques.  This is proof -- if any were needed -- that bad SQL is worse than no SQL.&lt;/p&gt;
&lt;p&gt;The table appears to have 2 columns, a date and a floating-point value in the range 0.0 to 1.0.  Rows arrive at the rate of 500 an hour.&lt;/p&gt;
&lt;p&gt;Someone wants a weekly summary (about 90,000 rows) binned into 10 ranges 0.0 to 0.1, 0.1 to 0.2, 0.2 to 0.3, etc.  The algorithm might be slightly more complex (to separate &lt;span class="formula"&gt;&lt;i&gt;n&lt;/i&gt; = 0.1&lt;/span&gt; from &lt;span class="formula"&gt;0.1 &amp;lt; &lt;i&gt;n&lt;/i&gt; ≤ 0.2&lt;/span&gt;.)&lt;/p&gt;
&lt;p&gt;[Again, the DBA steadfastly refuses to provide the use cases, so I'm doing a lot of this with minimal information.  However, I did get a spreadsheet showing an Excel version of the algorithm.  Not PL/SQL, not pure SQL, not Java, but Excel.]&lt;/p&gt;
&lt;div class="section" id="what-s-the-issue"&gt;
&lt;h2&gt;What's the Issue?&lt;/h2&gt;
&lt;p&gt;The issue is that some programmers can't be trusted to find their ass groping with both hands.  I was sent three versions of the obvious SQL query, each more contrived and senseless than the last.&lt;/p&gt;
&lt;p&gt;I was asked -- really -- &amp;quot;What is a more scalable approach to the problem ?&amp;quot;.  &amp;quot;Scalable&amp;quot;? WTF?  Scalable with respect to what?  Rows?  Physical I/O's?  Elapsed Time?  CPU use?  User queries?  Web page hits?  Shots of Tequila?  If I look at the &lt;a class="reference external" href="http://www.zifa.com/"&gt;Zachman Framework&lt;/a&gt;  or the &lt;a class="reference external" href="http://www.sei.cmu.edu/str/taxonomies/view_qm_body.html"&gt;SEI Quality Measures Taxonomy&lt;/a&gt; , I can come up with at least a half-dozen more dimensions of potential &amp;quot;scalability&amp;quot;.&lt;/p&gt;
&lt;p&gt;[Yes, I asked.  No, I didn't get an answer.  &amp;quot;Scalability&amp;quot; appears to mean the same thing as &amp;quot;Better&amp;quot;.]&lt;/p&gt;
&lt;p&gt;I'm assuming that the volume has increased or something, and the old query isn't fast enough.  Or something.  There's a claim that some query is run 83 million times each week; that's 138 times per second, a number I just don't believe.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fetishize-a-feature"&gt;
&lt;h2&gt;Fetishize a Feature&lt;/h2&gt;
&lt;p&gt;Someone has an Oracle Bulk Bind fetish.  I've listened to this tripe before.  There are probably places where it helps.  I haven't seen any, but I haven't really made a study of the feature.   Apparently, they couldn't get it to work for the required 80,000 rows.  It gets what they call &amp;quot;the standard ORA-04030 error.&amp;quot;&lt;/p&gt;
&lt;p&gt;The sent me a copy of solution one: a big pile of PL/SQL including some BULK COLLECT stuff.  PL/SQL they couldn't get it to work.  I'm not sure what's going on here, but it's clearly the first dumb-as-a-post SQL programming technique:  &lt;strong&gt;Fetishize a Feature&lt;/strong&gt;.  You pick something and stick with it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="drown-it-in-documentation"&gt;
&lt;h2&gt;Drown it in Documentation&lt;/h2&gt;
&lt;p&gt;Here's the best part of solution one.  It didn't work.  And they provided me with extensive documentation -- on the feature they couldn't get to work.  I like that.  So technique two is to quote a lot of documentation -- as if &lt;strong&gt;Drowning It In Documentation&lt;/strong&gt;  somehow make the feature start working.&lt;/p&gt;
&lt;p&gt;I suppose I could try and debug it, but I really don't have the patience.  There are simpler, provably faster techniques.  Why debug something that is highly Oracle-specific, and doesn't seem to work very well?&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="write-more-code"&gt;
&lt;h2&gt;Write More Code&lt;/h2&gt;
&lt;p&gt;Solution two was to purge the bulk-bind syntax from solution one and see if a big pile of PL/SQL will work.  PL/SQL is a demonstrably slow platform.  There are some anecdotal stories of applications that were made faster by replacing external application programs with PL/SQL.  I believe that those stories involve comparing the performance of a Bentley with an Etap 37S.  One's a car, the other's a boat.  PL/SQL is faster when you change your application design to make better use of PL/SQL features.&lt;/p&gt;
&lt;p&gt;In this case, the PL/SQL solution is a huge amount of code for something that is -- as far as I can tell -- a SELECT COUNT(*) GROUP BY operation.  It's hard to be completely sure, since the code is bad, and obscures the intent.&lt;/p&gt;
&lt;p&gt;Rather than summarize and simplify, they &lt;strong&gt;Wrote More Code&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="don-t-do-the-obvious"&gt;
&lt;h2&gt;Don't Do The Obvious&lt;/h2&gt;
&lt;p&gt;Another generally dumb technique is to avoid writing the obvious SQL because -- well -- I don't know why.  I don't have the actual requirements.  However, each example strives to produce one line of output with the frequency table spread out horizontally.  This is fairly hard to do in SQL, and requires lots of copy and paste programming to repeat the CASE expressions over and over again.&lt;/p&gt;
&lt;p&gt;The basic SELECT COUNT(*) GROUP BY produces a number of rows, each of which has a key and a count.  This can be rotated into a horizontal configuration by a reporting program.  For some reason, we're locked into a single form for the report, making it so we can't &lt;strong&gt;Do The Obvious&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="refuse-to-change-the-structure"&gt;
&lt;h2&gt;Refuse to Change the Structure&lt;/h2&gt;
&lt;p&gt;Data structures and algorithms are two complementary sides of the same coin.  You can't fix the algorithm without fixing the data structure, and vice versa.  In this case, the table design was bad, but no one seemed prepared to fix it.&lt;/p&gt;
&lt;p&gt;About a year ago, I had told a member of data cartel to read Ralph Kimball's Data Warehouse Toolkit.  They claimed they read it.  Since they got nothing out of it, I'm not sure what they meant by &amp;quot;read&amp;quot;.  Data warehouse folks know that you have to denormalize for reporting.  A relentless focus on &amp;quot;normalization&amp;quot; -- when dealing with non-updatable reporting-only data -- is simply wrong.&lt;/p&gt;
&lt;p&gt;In this case, the floating point numbers had to be split up into bins.  The calculation must be done at load time, and must be a permanent part of the table.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
TABLE data(
time DATETIME,
value FLOAT );
&lt;/pre&gt;
&lt;p&gt;This isn't really sufficient for reporting.  You need something more like the following:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
TABLE data(
time DATETIME,
week INTEGER,
month INTEGER,
year INTEGER,
value FLOAT,
bin INTEGER );
&lt;/pre&gt;
&lt;p&gt;The various derived values are all trivial to calculate at load time.  Once they're calculated, your query reduces to a trivial SELECT bin, COUNT(*) FROM DATA GROUP BY bin.  It isn't the absolutely fastest way to process the data, but it's a far, far sight faster than on-the-fly CASE expressions or PL/SQL loops.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-hubris-of-time-calculations"&gt;
&lt;h2&gt;The Hubris of Time Calculations&lt;/h2&gt;
&lt;p&gt;There's more that's wrong in the various examples I was sent.   Specifically, they use &amp;quot;closed-ended date ranges&amp;quot;.  A serious mistake that is caused by simple hubris.  Time is subtle and complex and easy to get wrong.&lt;/p&gt;
&lt;p&gt;Here's their code.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
time &amp;gt;= TO_DATE( '05/01/2008 00:00:00', 'MM/DD/YYYY HH24:MI:SS') AND
time &amp;lt;= TO_DATE( '05/08/2008 23:59:59', 'MM/DD/YYYY HH24:MI:SS');
&lt;/pre&gt;
&lt;p&gt;It can't -- in general -- work.&lt;/p&gt;
&lt;p&gt;There's a 1-second gap between the two times.  You have use half-open intervals to avoid losing a row that happens to have a timestamp in the gap.  [Don't waste time adding .999's, either, because the decimal value doesn't provide down-to-the-last bit way to encode the internal binary values.]&lt;/p&gt;
&lt;pre class="literal-block"&gt;
time &amp;gt;= TO_DATE('05/01/2008','MM/DD/YYYY')
AND time &amp;lt; TO_DATE('05/08/2008','MM/DD/YY' )
&lt;/pre&gt;
&lt;p&gt;This has NO gap.&lt;/p&gt;
&lt;p&gt;However, this still isn't very good.  As shown in the table definitions above, you need to denormalize the time-stamp into the buckets you actually want to use for selection and grouping.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="real-speed"&gt;
&lt;h2&gt;Real Speed&lt;/h2&gt;
&lt;p&gt;I don't have the table or sample data, so I can't compare my results with their performance numbers.  However, their numbers are sad.&lt;/p&gt;
&lt;p&gt;First, they couldn't get the bulk bind to work, but sent me the code, as if it mattered.&lt;/p&gt;
&lt;p&gt;Second, their massive PL/SQL loop ran for an hour.  Apparently, this is unacceptable, but they sent me the code, as if it mattered.  Which is sad.&lt;/p&gt;
&lt;p&gt;Third, their SQL GROUP-BY with all the CASE expressions ran in 12 minutes.  I don't know if that's too long or uses too much memory or takes too many tequila shots.&lt;/p&gt;
&lt;p&gt;The real SELECT COUNT(*) GROUP BY, with denormalized data, is fast.  On my little 1Gb RAM, 1.7Ghz Dell, running Fedora Core 8 and using SQLite, a basic SELECT COUNT(*) processes 100,000 records in about 3 seconds.&lt;/p&gt;
&lt;p&gt;That's about as fast as this little drip of code.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
import collections
    count= collections.defaultdict(lambda:0)
    for row in q.execute().fetchall():
        b, exact = divmod( int(row[1]*100), 10 )
        band= &amp;quot;==0.%d&amp;quot;%(b,) if exact == 0 else &amp;quot;0.%d-0.%d&amp;quot;%(b,b+1)
        count[band] += 1
    print count
&lt;/pre&gt;
&lt;p&gt;In SQLite, for 100,000 rows, this is the same speed as SQL.  Why?  Because we're not asking the database to do anything much more than fetch rows.&lt;/p&gt;
&lt;p&gt;Interestingly, in Oracle, the &lt;tt class="docutils literal"&gt;SELECT &lt;span class="pre"&gt;COUNT(*)&lt;/span&gt; GROUP BY&lt;/tt&gt; is much, much faster.  Why?  Because Oracle queries involve a context switch, where SQLite does not.  A simple fetch loop in Oracle is relatively slow without using some kind of buffering.&lt;/p&gt;
&lt;p&gt;The database fetch time still dominates what we're doing.  A table design change, and doing all processing at load time will minimizes the query time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-many-bad-things-can-we-do"&gt;
&lt;h2&gt;How Many Bad Things Can We Do?&lt;/h2&gt;
&lt;p&gt;Let's enumerate them:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Fetishize a Feature&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Drown It In Documentation&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write More Code&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Refuse to Change the Structure&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Hubris of Time Calculation&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of these habits get in the way of a simple denormalization that makes the obvious query work at amazing speeds.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>The Django World-View: Model+Admin First; Built-in Transparency and Trustworthiness</title><link href="https://slott56.github.io/2008_03_24-the_django_world_view_modeladmin_first_built_in_transparency_and_trustworthiness.html" rel="alternate"></link><published>2008-03-24T18:24:00-04:00</published><updated>2008-03-24T18:24:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2008-03-24:/2008_03_24-the_django_world_view_modeladmin_first_built_in_transparency_and_trustworthiness.html</id><summary type="html">&lt;p&gt;See Michael Hugos &amp;quot;&lt;a class="reference external" href="http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;amp;articleId=314557"&gt;Think about screens and the data on them to simplify system development&lt;/a&gt; &amp;quot; for some helpful insight on what an &amp;quot;application&amp;quot; really is -- access to data.  Simple transparency is lifted up as a critical value for software.&lt;/p&gt;
&lt;p&gt;I liked the &amp;quot;If you don't believe it could be this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;See Michael Hugos &amp;quot;&lt;a class="reference external" href="http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;amp;articleId=314557"&gt;Think about screens and the data on them to simplify system development&lt;/a&gt; &amp;quot; for some helpful insight on what an &amp;quot;application&amp;quot; really is -- access to data.  Simple transparency is lifted up as a critical value for software.&lt;/p&gt;
&lt;p&gt;I liked the &amp;quot;If you don't believe it could be this simple, consider the reasons for your response&amp;quot; insight.  I didn't like Hugos' sample response, &amp;quot;complex code that you can brag about&amp;quot;.  I don't think complexity for the sake of complexity is a real problem.&lt;/p&gt;
&lt;p&gt;Complexity can be defined as everything that separates the user from their data.  As Hugos' notes, the data model and the simplest, most direct presentation is the best design.  Everything else is complexity that obscures the real purpose of the software.&lt;/p&gt;
&lt;p&gt;Complexity comes from several sources.  I've blogged about complexity &lt;a class="reference external" href="https://slott56.github.io/2005_09_03-why_are_things_so_complicated_7_deadly_reasons.html"&gt;before&lt;/a&gt; , ever since &lt;a class="reference external" href="http://www.mindspring.com/~mgrand/"&gt;Mark Grand&lt;/a&gt;  gave me the hint that complexity was part of the IT culture.&lt;/p&gt;
&lt;p&gt;Here are seven kinds of complexity that get between a user and their data.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;strong&gt;&amp;quot;The Conflict Is The Problem&amp;quot;&lt;/strong&gt;. The inherent conflicts in the relationship between developers and buyers or users make the problem appear complex. Often this is because buyers insist on their solution -- irrespective of the actual problem.  Rather than describe the underlying problem, buyers describe a solution based on their favorite technology.  They insist they have to do this because the job of the business analyst is to translate the business problem to technology terms -- usually oriented around a complex non-solution.  After all, when you try to solve a business problem with a spreadsheet, you've created two business problems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;quot;Fear of Showing Weakness&amp;quot;&lt;/strong&gt;. Simplicity isn't valued.  Some aspect of the problem is (or appears) complex, so we need lots of complex software.  Many &amp;quot;business rules&amp;quot; are transient; an orientation around the decisions a person needs to make is more helpful than over-specifying something that handles 1% of the dollar value of an application.  Rather than simply expose the data (and the business process) to the people, we overdesign &amp;quot;automation&amp;quot; that makes the exceptions and special cases a larger and more complex problem than they deserve to be.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;quot;Quality vs. Quantity of Ideas&amp;quot;&lt;/strong&gt;. It's hard to let go of the first idea, no matter how bad it is.  Someone with deep experience in legacy technology will often be the root cause of complexity.  Just because batch processing was once the vogue doesn't mean it is essential or even necessary.  Many, many things can be handled via an asynchronous message queue rather than an overnight batch process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;quot;Form vs. Function&amp;quot;&lt;/strong&gt;. If we fail to define the problem in the first place, we don't know what problem we're solving.  We're left applying technology inappropriately, filling in the form of a solution, manufacturing complexity because we're vague on what the actual function should be.  Rather than simply present data, we feel that application logic is &amp;quot;important&amp;quot; and should be part of the system; simple presentation of data isn't appropriate.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&amp;quot;When I Grow Up&amp;quot;&lt;/strong&gt;. If we don't have a mature process for solving problems, we're stuck applying inappropriate technology.  Often we're forced into building something prematurely, and the previous problems all surface: we lock onto the first bad idea, we don't back down from that bad idea, it fills the form of software we think we understand.&lt;/li&gt;
&lt;li&gt;&amp;quot;&lt;strong&gt;If I Had A Hammer&amp;quot;&lt;/strong&gt;. Tied in with the lack of quality ideas or well-defined problems, we make inappropriate use of tools or solution design patterns; we view all fastener problems as nails because we only understand hammers.  We have very, very sophisticated application software development tools; we don't need to write mountains of code when we have sophisticated technology stacks like Linux/Apache/MySQL/Python and Django.&lt;/li&gt;
&lt;li&gt;&amp;quot;&lt;strong&gt;How Hard Can It Be?&amp;quot;&lt;/strong&gt; Failure to assess risks appropriately biases users against a simple solution.  More programming seems -- in some views -- to be less risky.  More automation isn't a solution.  Appropriate controls are more important than volume of software.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Read Thibodeau's &amp;quot;&lt;a class="reference external" href="http://www.computerworld.com/action/article.do?command=viewArticleBasic&amp;amp;articleId=9066618"&gt;D.C.'s tax system won plaudits but couldn't stop alleged insider thefts&lt;/a&gt; &amp;quot;.  Complexity and technology aren't the answer.  Good old-fashioned controls and audits are what matters.  Audits and controls require transparency, not complexity.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>How Essential Is Unit Testing? Or, How Do We Make It Essential?</title><link href="https://slott56.github.io/2007_12_24-how_essential_is_unit_testing_or_how_do_we_make_it_essential.html" rel="alternate"></link><published>2007-12-24T11:31:00-05:00</published><updated>2007-12-24T11:31:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2007-12-24:/2007_12_24-how_essential_is_unit_testing_or_how_do_we_make_it_essential.html</id><summary type="html">&lt;p&gt;See &lt;a class="reference external" href="http://thomas.apestaart.org/log/?p=559"&gt;Present Perfect&lt;/a&gt;  for some thoughts on unit testing.   See some other commentary on the discipline required to write Python programs in &lt;a class="reference external" href="http://panela.blog-city.com/gnome_devs_too_lazy_for_python.htm"&gt;Gnome devs too lazy for python&lt;/a&gt; .  I think I see the disconnect that makes testing appear to be too costly; I think that some basic &amp;quot;meta-quality attributes&amp;quot; are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;See &lt;a class="reference external" href="http://thomas.apestaart.org/log/?p=559"&gt;Present Perfect&lt;/a&gt;  for some thoughts on unit testing.   See some other commentary on the discipline required to write Python programs in &lt;a class="reference external" href="http://panela.blog-city.com/gnome_devs_too_lazy_for_python.htm"&gt;Gnome devs too lazy for python&lt;/a&gt; .  I think I see the disconnect that makes testing appear to be too costly; I think that some basic &amp;quot;meta-quality attributes&amp;quot; are essential to understanding unit testing.&lt;/p&gt;
&lt;p&gt;Here's the original C# vs. Python analysis in &lt;a class="reference external" href="http://joeshaw.org/2007/10/28/496"&gt;Monotonous&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;Here's the famous quote: &amp;quot;Writing real applications in Python requires a discipline that unfortunately most people (including myself, at that time) are unwilling to adhere to, and this easily leads to buggy and hard to maintain programs. You have to be very diligent about unit tests and code coverage for every line of code, because you can’t rely on the compiler to catch errors for you.&amp;quot;&lt;/p&gt;
&lt;p&gt;I didn't take careful notes at a client meeting where this came up, so I don't have good quotes.  The client balked at the very idea of Test Driven Development.  In a larger presentation on testing (technology, environments, process, etc.) I had lifted up TDD as a direction that would benefit the developers.  The response from the director of development was a series of &amp;quot;how would you do that?&amp;quot; questions.&lt;/p&gt;
&lt;p&gt;These weren't practical &amp;quot;how to&amp;quot; questions.  They were rhetorical &amp;quot;that isn't possible&amp;quot; statements, framed as questions.  In order to portray TDD as impossible the questions quickly devolved into how TDD interacts with requirements gathering and business analysis; I couldn't successfully bracket the questions as part of the fringe of TDD.  I think the disconnect was their certain knowledge that test cases come only from requirements and nowhere else.&lt;/p&gt;
&lt;div class="section" id="it-hurts-when-i-do-that"&gt;
&lt;h2&gt;It Hurts When I Do That&lt;/h2&gt;
&lt;p&gt;TDD is -- certainly -- a pain the neck.  I think I see two complaints.  First, it's a lot of &amp;quot;extra&amp;quot; code.  I'm guessing that there's a &amp;quot;non-deliverable&amp;quot; view of test cases that pervades some people's thinking.  I've been measuring the lines of code in both parts of a project, and the total volume of source is about 50% test cases and 50% operational.&lt;/p&gt;
&lt;p&gt;Many years ago, we made a distinction between &amp;quot;deliverable&amp;quot; and &amp;quot;non-deliverable&amp;quot; software.  We used to carefully segregate any non-deliverable software so that we could wring our hands over how to estimate the cost for it.  Since it wasn't &amp;quot;deliverable,&amp;quot; some managers felt we couldn't charge the customer for it; the logical conclusion was that we should exclude it from our project plans.&lt;/p&gt;
&lt;p&gt;I maintained that &amp;quot;non-deliverable&amp;quot; is still &amp;quot;essential&amp;quot;, so we must include it in our plans.  The &amp;quot;compromise&amp;quot; was to inflate the estimated size of the deliverable to include a pro-rated version of the non-deliverable code.  The claim was that non-deliverable software was half the cost of deliverable software.  It had less documentation and less testing or some such.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="i-can-t-cope-with-that"&gt;
&lt;h2&gt;I Can't Cope With That&lt;/h2&gt;
&lt;p&gt;The second complaint seems to be that programmers can't be trusted.  Because they can't be trusted, we must have full definitions of all deliverables, complete up-front design, rigorous schedules for code creation, and a fungible, compressible schedule for testing.&lt;/p&gt;
&lt;p&gt;We can't engage in an agile test-driven process for a number of reasons.  First, and foremost, programmers are the root cause of scope creep.  We all know that programmers will &amp;quot;gold-plate&amp;quot; the simplest thing; they'll spend years polishing and improving something of limited business value.  [Why did we let them get started on something of limited value?  Why aren't we willing to invest in making it work correctly every time?]&lt;/p&gt;
&lt;p&gt;We can't trust programmers to do just enough design because they are lazy slobs and won't ever get anything to work.  If we try to let them fire at half-cock, they'll never get anything useful accomplished. After all, we -- as managers -- have a vision of something trivially simple.  The programmers keep introducing some technology nuance that makes a simple thing horribly complex and difficult.  [Who -- specifically -- told us that introducing new technology will be simpler?  Or did we just make that part up?]&lt;/p&gt;
&lt;p&gt;We can't trust programmers to evolve code, design and test hand-in-hand.  If we did, there'd be scope creep and they'd just play with the technology.  Worse, of course, they'd miss the schedule.&lt;/p&gt;
&lt;p&gt;We certainly can't trust programmers and end-users to collaborate.  If we did, they would change the focus of the project, and the schedule might be missed.  As managers, we don't fully understand the business value proposition; we don't completely get the technology, but we do understand the schedule.  Since we really, truly, deeply understand the calendar, that is the one thing we can manage to.  [Why is schedule more important than delivered features?]&lt;/p&gt;
&lt;p&gt;Above all, we can't trust programmers to create test cases.  Only end-users can create tests, and those tests must be married to the requirements.  There's no reason to elaborate the tests to match the design, or elaborate the tests to match the details embodied in the code.  Tests based on design or programming amount to letting a programmer do their own tests; programmers are untrustworthy; therefore this can't work.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-tdd-alternatives"&gt;
&lt;h2&gt;The TDD Alternatives&lt;/h2&gt;
&lt;p&gt;I think there's just one disconnect underlying this.  This disconnect manifests itself as two alternatives to TDD.  The static language folks seem to like the idea that compiler type-checking is an alternative to testing.  I suppose -- to a limited extent -- this is true.  Rather than write a unit test to examine proper integration among classes or modules, we can trust the compiler.&lt;/p&gt;
&lt;p&gt;Everyone knows -- or should know -- that the compiler is easily fooled.  When using externally developed JAR files, we can easily compile against one version, and try to execute against a different version.  All the compile-time type-checking in the world can't cope with mis-configuration.  Eventually, we need Python-style dynamic testing.&lt;/p&gt;
&lt;p&gt;When we can't trust our programmers, we have a number of clever alternatives to TDD.  The primary approach is to define a process that imposes a waterfall approach to developing unit test cases in parallel with the code.  When asked &amp;quot;How does TDD interact with requirements gathering?&amp;quot; no answer I could give was acceptable.  What they wanted me to say was &amp;quot;Oh crap, you're right, I'm such an idiot.  Test cases are only based on requirements, never design or programming.&amp;quot;  They wanted me to agree that programmers can't be allowed write their own test cases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-disconnect"&gt;
&lt;h2&gt;The Disconnect&lt;/h2&gt;
&lt;p&gt;I think both viewpoints stem from looking at testing as &amp;quot;final&amp;quot;, &amp;quot;end-user&amp;quot; or &amp;quot;acceptance&amp;quot; testing.  If testing is only for acceptance, then we should have other ways to test that the programming is correct and the design really works.  The compiler should -- somehow -- validate our basic programming via static type analysis.  The design, similarly, should be checked via some static analysis.  That leaves testing to focus on the requirements and nothing else.&lt;/p&gt;
&lt;p&gt;Additionally, we can't trust one person to interpret the requirements as test cases and as code.  We must apply a second pair of eyeballs to the requirements to create the acceptance-oriented test cases.&lt;/p&gt;
&lt;p&gt;It appears to me that TDD is dismissed as worthless because people don't see a need to test their designs or programming.  Either they hope that static type analysis will do this, or they simply dismiss this testing as worthless.&lt;/p&gt;
&lt;p&gt;It's hard to create a value proposition for testing the design and programming.  It requires emphasizing a sense of distrust.  And the level of distrust is already fairly high.  After all, unit testing requires a level of discipline that programmers are unwilling to adhere to.  The idea of adding testing at the design and code level only gets into complex philosophical discussions about &amp;quot;the role of requirements&amp;quot;.&lt;/p&gt;
&lt;p&gt;It's hard to break testing free from &amp;quot;End-User Acceptance.&amp;quot;  However, if we can portray testing as essential, it then becomes deliverable.  Indeed, it becomes essential to establishing confidence in the software.  It also becomes part of the documentation, since each API is demonstrated by at least a test case (in some cases, a whole test suite.)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="meta-quality"&gt;
&lt;h2&gt;Meta-Quality&lt;/h2&gt;
&lt;p&gt;The logical conclusion is a set of meta-quality attributes.  Software quality attributes can be based on the SEI Quality Measures Taxonomy &lt;a class="reference external" href="http://www.sei.cmu.edu/str/taxonomies/view_qm.html"&gt;http://www.sei.cmu.edu/str/taxonomies/view_qm.html&lt;/a&gt;.  This taxonomy includes need satisfaction, resource use, maintainability, adaptability and cost factors.&lt;/p&gt;
&lt;p&gt;Meta-quality includes the quality attributes of the test cases.  There are probably a number of quality attributes regarding things like 'traceability to requirements&amp;quot;, &amp;quot;class coverage&amp;quot; and &amp;quot;method coverage&amp;quot; that determine how useful and complete the test cases are.  Looking at &amp;quot;traceability&amp;quot;, we examine how the test cases apply to end-user acceptance.&lt;/p&gt;
&lt;p&gt;I look at &amp;quot;class coverage&amp;quot; as  a way to to look at the design.  This includes classical &amp;quot;class-in-isolation&amp;quot; unit tests, as well as module- (or &amp;quot;component&amp;quot; or &amp;quot;package&amp;quot;) -level unit tests that examine a collection of classes to be sure that they interact properly.  This makes limited use of mock objects, since this is looking at integration of classes and modules.&lt;/p&gt;
&lt;p&gt;The &amp;quot;method coverage&amp;quot; is how we look at the programming.  This includes appropriate test cases to exercise each method more-or-less in isolation.  This level of testing makes heavy use of mock objects to be sure that the code in each method is actually correct.&lt;/p&gt;
&lt;p&gt;I think that these meta-quality attributes of the test case code is as important as the quality attributes of the &amp;quot;operational&amp;quot; code.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>User Interface Testing</title><link href="https://slott56.github.io/2007_08_14-user_interface_testing.html" rel="alternate"></link><published>2007-08-14T10:34:00-04:00</published><updated>2007-08-14T10:34:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2007-08-14:/2007_08_14-user_interface_testing.html</id><summary type="html">&lt;p&gt;The question seemed simple, which testing framework is the simplest?  The situation is complex.  There's a web application, there are developers and there are testers.  The developers develop, and the testers test.  So far, not so complex.&lt;/p&gt;
&lt;p&gt;Here's the complexity.  The testers are pretty focused on manual point-and-click testing.  They …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The question seemed simple, which testing framework is the simplest?  The situation is complex.  There's a web application, there are developers and there are testers.  The developers develop, and the testers test.  So far, not so complex.&lt;/p&gt;
&lt;p&gt;Here's the complexity.  The testers are pretty focused on manual point-and-click testing.  They didn't like &lt;a class="reference external" href="http://httpunit.sourceforge.net/"&gt;HttpUnit&lt;/a&gt; , declaring it too complex.&lt;/p&gt;
&lt;div class="section" id="what-s-simpler-than-httpunit"&gt;
&lt;h2&gt;What's simpler than HttpUnit?&lt;/h2&gt;
&lt;p&gt;At first blush, my answer was to look at &lt;a class="reference external" href="http://www.openqa.org/selenium/"&gt;Selenium&lt;/a&gt; .  This is a widely-used, easily automated toolset for browser and UI testing.  But further conversation showed that this is the wrong approach.&lt;/p&gt;
&lt;p&gt;They aren't deeply interested in the kind of cross-browser testing that Selenium does well.  They're more interested in the essential functionality testing that HttpUnit does.  They need to know that the application works with the given target browser.  Articles like &amp;quot;&lt;a class="reference external" href="http://magpiebrain.com/blog/2007/01/28/selenium-rocks-and-you-dont-need-it/"&gt;Selenium rocks - and you don't need it&lt;/a&gt; &amp;quot; help to clarify this distinction between Selenium and HttpUnit&lt;/p&gt;
&lt;p&gt;My next answer was to look at &lt;a class="reference external" href="http://twill.idyll.org/"&gt;Twill&lt;/a&gt; .  Articles like the Advogato &amp;quot;&lt;a class="reference external" href="http://www.advogato.org/article/874.html"&gt;Introduction&lt;/a&gt; &amp;quot; are very compelling.&lt;/p&gt;
&lt;p&gt;It turns out, though, the real problem isn't &amp;quot;complexity&amp;quot; &lt;em&gt;per se&lt;/em&gt; .  The real problem is that the testers aren't interested in writing sophisticated test scripts.  They know the application, they know what they want to see, and they don't feel that programming is the best use of their time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="unit-testing-101"&gt;
&lt;h2&gt;Unit Testing 101&lt;/h2&gt;
&lt;p&gt;This wasn't my idea, I'm just relaying the insight I got from the conversation.  I was busy shilling shamelessly for Twill when the real solution surfaced.&lt;/p&gt;
&lt;p&gt;The smart answer isn't to give the testers more tools.  The testers (as currently managed) don't see a need for tools.  The smart answer is to have the developers made officially responsible for unit tests, in HttpUnit (or Twill).  The developers need to put the unit tests into the source tree along with everything else.  They need to run the unit tests themselves.&lt;/p&gt;
&lt;p&gt;The official &amp;quot;testers&amp;quot; are now freed from the &amp;quot;test everything&amp;quot; requirement.  Instead, they can now do &amp;quot;guerilla testing&amp;quot; as well as review the unit test logs.&lt;/p&gt;
&lt;p&gt;At some point in time -- and at a higher level in the organization -- the testers need to be encouraged to use powerful scripting and unit testing tools as force multipliers.  They can claim that HttpUnit is too complex, but that's because they're looking at the wrong thing.&lt;/p&gt;
&lt;p&gt;They need to see that they can only point and click so fast.  A tool like Twill or HttpUnit can point and click a whole lost faster.  Until they're rewarded for speed, they don't have any incentive to master a tool.  Until they're given the incentive, every tool will be labeled as &amp;quot;too complex&amp;quot;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>Another Dimensional Model Implementation</title><link href="https://slott56.github.io/2007_05_26-another_dimensional_model_implementation.html" rel="alternate"></link><published>2007-05-26T01:14:00-04:00</published><updated>2007-05-26T01:14:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2007-05-26:/2007_05_26-another_dimensional_model_implementation.html</id><summary type="html">&lt;p&gt;The &lt;a class="reference external" href="http://sourceforge.net/projects/cubulus/"&gt;Cubulus&lt;/a&gt;  project and &lt;a class="reference external" href="http://alxtoth.webfactional.com/"&gt;Alexandru Toth&lt;/a&gt; 's page describe an &amp;quot;OLAP Aggregation Engine&amp;quot;.  It is very nice to see advanced work done on the dimensional model.&lt;/p&gt;
&lt;p&gt;The cited research dates from 1999 (V. Markl, F. Ramsak, R. Bayer, &amp;quot;Improving OLAP Performance by Multidimensional Hierarchical Clustering&amp;quot;, &lt;em&gt;Proceedings of the Intl. Database …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a class="reference external" href="http://sourceforge.net/projects/cubulus/"&gt;Cubulus&lt;/a&gt;  project and &lt;a class="reference external" href="http://alxtoth.webfactional.com/"&gt;Alexandru Toth&lt;/a&gt; 's page describe an &amp;quot;OLAP Aggregation Engine&amp;quot;.  It is very nice to see advanced work done on the dimensional model.&lt;/p&gt;
&lt;p&gt;The cited research dates from 1999 (V. Markl, F. Ramsak, R. Bayer, &amp;quot;Improving OLAP Performance by Multidimensional Hierarchical Clustering&amp;quot;, &lt;em&gt;Proceedings of the Intl. Database Engineering and Applications Symposium&lt;/em&gt; , pp. 165-177, 1999.)  I'm suspicious that it predates the &amp;quot;bit-mapped index&amp;quot;.&lt;/p&gt;
&lt;p&gt;It may be that this technique helps a lot with an RDBMS that doesn't support the star schema via bit-mapped indexes.  It may be that this technique only helps a little with a more modern RDBMS.&lt;/p&gt;
&lt;p&gt;However, the idea of a nice, tidy Python application that helps manipulate the dimensional model is a great thing.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>Just for a moment, I though I'd found something SQLAlchemy doesn't do perfectly.</title><link href="https://slott56.github.io/2007_05_18-just_for_a_moment_i_though_id_found_something_sqlalchemy_doesnt_do_perfectly.html" rel="alternate"></link><published>2007-05-18T17:40:00-04:00</published><updated>2007-05-18T17:40:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2007-05-18:/2007_05_18-just_for_a_moment_i_though_id_found_something_sqlalchemy_doesnt_do_perfectly.html</id><summary type="html">&lt;p&gt;After having written a number of application-specific object-relational mappers, I have been on the prowl for an elegant, enduring solution.  I had started to come to grips with &lt;a class="reference external" href="http://www.djangoproject.com/"&gt;Django&lt;/a&gt; , and like much of the approach.  Django has a tiny infrastructure feature (the settings.py file) which made it unpleasant to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After having written a number of application-specific object-relational mappers, I have been on the prowl for an elegant, enduring solution.  I had started to come to grips with &lt;a class="reference external" href="http://www.djangoproject.com/"&gt;Django&lt;/a&gt; , and like much of the approach.  Django has a tiny infrastructure feature (the settings.py file) which made it unpleasant to separate the ORM from the rest of the framework.  (Not impossible, just fleetingly unpleasant.)&lt;/p&gt;
&lt;p&gt;My first look at SQLAlchemy made it look over the top.  However, after the PyCon 2007 presentation, I realized that the layers were cleanly separated, and I could use the ORM without messing about in the SQL-in-Python-Notation layer.&lt;/p&gt;
&lt;p&gt;Then, I figured out (&amp;quot;&lt;a class="reference external" href="../C465799452/E20070322201220/index.html"&gt;PL/SQL vs. Java, Which One is Really Faster?&lt;/a&gt; &amp;quot;) that stored procedures were slow.  Given that PL/SQL is slow, what else in the RDBMS world is slow?  How much SQL is too much SQL, when speed matters?  That answer is forthcoming -- I'm still fussing around with experiments.&lt;/p&gt;
&lt;div class="section" id="problem-child"&gt;
&lt;h2&gt;Problem Child&lt;/h2&gt;
&lt;p&gt;The central issue started out as the all-too-common situation of &lt;strong&gt;Disjoint Subentities&lt;/strong&gt;.  This is where a single table has distinct classes of entities.  The usual symptoms of this are indicators or NULL columns.  Often, both are used.  Sometimes, the indicator is omitted, and the pattern of NULLs has to be used to discriminate among the entity classes.&lt;/p&gt;
&lt;p&gt;In this specific experiment, a single table has two subentities, each with different granularity.  One subentity has to be summed to match the grain of the other.  This gives us a union of two kinds of SQL queries: detailed and summary.&lt;/p&gt;
&lt;p&gt;The detailed query, in SQLAlchemy, looks like this:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
qrySingle= select(
    [stuff.c.groupName,stuff.c.amount,literal(1)],
    and_(stuff.c.status=='unmatched',
        stuff.c.subtype=='single'))
&lt;/pre&gt;
&lt;p&gt;In SQL, this is&lt;/p&gt;
&lt;pre class="literal-block"&gt;
SELECT &amp;quot;stuff&amp;quot;.&amp;quot;groupName&amp;quot;, &amp;quot;stuff&amp;quot;.amount, ?
FROM &amp;quot;stuff&amp;quot; WHERE &amp;quot;stuff&amp;quot;.status = ? AND &amp;quot;stuff&amp;quot;.subtype = ?
&lt;/pre&gt;
&lt;p&gt;This is precisely the SQL that would be coded &amp;quot;by hand&amp;quot;.  The literals (1, 'unmatched' and 'single') are bound into the SQL at run-time.&lt;/p&gt;
&lt;p&gt;The summary query looks like this in SQLAlchemy.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
qryMulti= select(
     [stuff.c.groupName,func.sum(stuff.c.amount),literal(2)],
     and_(stuff.c.status=='unmatched',
             stuff.c.subtype=='multi'),
     group_by=[stuff.c.groupName])
&lt;/pre&gt;
&lt;p&gt;And produces the following SQL.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
SELECT &amp;quot;stuff&amp;quot;.&amp;quot;groupName&amp;quot;, sum(&amp;quot;stuff&amp;quot;.amount), ?
FROM &amp;quot;stuff&amp;quot;
WHERE &amp;quot;stuff&amp;quot;.status = ? AND &amp;quot;stuff&amp;quot;.subtype = ?
GROUP BY &amp;quot;stuff&amp;quot;.&amp;quot;groupName&amp;quot;
&lt;/pre&gt;
&lt;p&gt;This is all very pleasant.  You can see that the literals (2, 'unmatched', 'multi') are bound in at run-time.  This technique often leads to a speed-up because the SQL statement can be reused by the RDBMS.  When coding by hand, this is easily overlooked.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="real-world"&gt;
&lt;h2&gt;Real World&lt;/h2&gt;
&lt;p&gt;In the &amp;quot;real world&amp;quot;, that is, the world of my clients, this kind of query is distressingly common.  And doing simulations and architectural recommendations is often made complex by having to cope with these kind of table designs.&lt;/p&gt;
&lt;p&gt;To work with this table, I needed a union, and (for a brief time) SQLAlchemy couldn't generate the correct SQL.&lt;/p&gt;
&lt;p&gt;Here's my union in SQLAlchemy.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
invQry= union( qrySingle, qryMulti )
&lt;/pre&gt;
&lt;p&gt;Here's the SQL which was generated.  Note that the GROUP-BY vanished.&lt;/p&gt;
&lt;pre class="literal-block"&gt;
SELECT &amp;quot;stuff&amp;quot;.&amp;quot;groupName&amp;quot;, &amp;quot;stuff&amp;quot;.amount, ?
FROM &amp;quot;stuff&amp;quot;
WHERE &amp;quot;stuff&amp;quot;.status = ? AND &amp;quot;stuff&amp;quot;.subtype = ?
UNION SELECT &amp;quot;stuff&amp;quot;.&amp;quot;groupName&amp;quot;, sum(&amp;quot;stuff&amp;quot;.amount), ?
FROM &amp;quot;stuff&amp;quot;
WHERE &amp;quot;stuff&amp;quot;.status = ? AND &amp;quot;stuff&amp;quot;.subtype = ?
&lt;/pre&gt;
&lt;p&gt;Very disappointing.  However, it's since been fixed.  And the amazing speed of that fix is more reason to love SQLAlchemy and the folks who support it.  Many thanks!&lt;/p&gt;
&lt;p&gt;Now we can continue investigating which is faster: &amp;quot;Pure SQL&amp;quot; (i.e., complex stored procedures) or some programming language which uses SQL as necessary for persistence.&lt;/p&gt;
&lt;/div&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>Dejavu and Python-based Dimensional Analysis</title><link href="https://slott56.github.io/2007_03_13-dejavu_and_python_based_dimensional_analysis.html" rel="alternate"></link><published>2007-03-13T10:14:00-04:00</published><updated>2007-03-13T10:14:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2007-03-13:/2007_03_13-dejavu_and_python_based_dimensional_analysis.html</id><summary type="html">&lt;p&gt;Actually, the code looks like a clever expansion
on my example, in &lt;a class="reference external" href="https://slott56.github.io/2007_02_26-pycon_2007_revised.html"&gt;PyCon 2007
(Revised)&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&amp;quot;But wait,&amp;quot; you
say.  &amp;quot;Creating a pivot table in
Python?&amp;quot;&lt;/p&gt;
&lt;p&gt;Of course.  Spreadsheets can
create pivot tables from dimensionally normalized data.  However, getting the
data in this form is often challenging and if there is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Actually, the code looks like a clever expansion
on my example, in &lt;a class="reference external" href="https://slott56.github.io/2007_02_26-pycon_2007_revised.html"&gt;PyCon 2007
(Revised)&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&amp;quot;But wait,&amp;quot; you
say.  &amp;quot;Creating a pivot table in
Python?&amp;quot;&lt;/p&gt;
&lt;p&gt;Of course.  Spreadsheets can
create pivot tables from dimensionally normalized data.  However, getting the
data in this form is often challenging and if there is any manual operation at
all, the data quality is immediately
suspect.&lt;/p&gt;
&lt;p&gt;To have perfect transparency
-- with no possibility of manual transformations -- you need a simple
application program which reliably, auditably, and testably produces the correct
data.  Further, you want to reduce the manual operations to formatting and
presentation.  The ideal solution is to produce the data in the required pivot
table so that it can be loaded into a spreadsheet for display
only.&lt;/p&gt;
&lt;p&gt;With an object-relational mapper,
you can write a tidy query to fetch raw data, and compute a aggregate along two
dimensions.  You then assemble result columns on one dimension and rows on the
other dimension.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Elegant -- But Dirty -- Pool.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The Elegant
Thing that makes this work pleasantly and simply in Python is being able to use
a tuple as the key to a mapping.  I can't say enough good things about this
simple, elegant piece of Pythonic programming.  You can easily handle complex,
multi-column keys in each dimension of the pivot table, by simply creating a
tuple of key values, and using a pair of tuples to locate the appropriate cell
in a mapping.&lt;/p&gt;
&lt;p&gt;Things like dimensional
conformance often create a gnarly algorithm in Java or -- shudder -- COBOL.  In
Python, it's a tuple that you can use to locate the dimension value in a
dictionary.  It works for everything except the Customer dimension, which in
some applications is too huge to retain in a simple in-memory
mapping.&lt;/p&gt;
&lt;p&gt;The graceful elegance of
&lt;strong&gt;Python's Mapping Indexed By A Tuple™&lt;/strong&gt;  (MXT) can really prevent a lot of
brain-cramping bugs.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>More Dimensional Model Implementations</title><link href="https://slott56.github.io/2007_03_03-more_dimensional_model_implementations.html" rel="alternate"></link><published>2007-03-03T00:59:00-05:00</published><updated>2007-03-03T00:59:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2007-03-03:/2007_03_03-more_dimensional_model_implementations.html</id><summary type="html">&lt;p&gt;The ActiveWarehouse project dates from
10/23/2006.&lt;/p&gt;
&lt;p&gt;I submitted a really
similar article to &lt;a class="reference external" href="http://www.ddj.com/"&gt;DDJ&lt;/a&gt;  in September.  I heard nothing and I don't
like to nag, so I let it drop.  Although last time I nagged, the article was
sprung free from where it had been spiked and wound …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The ActiveWarehouse project dates from
10/23/2006.&lt;/p&gt;
&lt;p&gt;I submitted a really
similar article to &lt;a class="reference external" href="http://www.ddj.com/"&gt;DDJ&lt;/a&gt;  in September.  I heard nothing and I don't
like to nag, so I let it drop.  Although last time I nagged, the article was
sprung free from where it had been spiked and wound up on the web site.  I
figured this article wasn't up to snuff, so I rewrote it for PyCon.  I submitted
it to PyCon on 10/10, with a pretty complete
outline.&lt;/p&gt;
&lt;p&gt;And then, wonderfully, someone
else does almost the same thing in Ruby.  I guess I was on the right
path.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>What a Data Warehouse Can Never Do</title><link href="https://slott56.github.io/2007_01_12-what_a_data_warehouse_can_never_do.html" rel="alternate"></link><published>2007-01-12T14:40:00-05:00</published><updated>2007-01-12T14:40:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2007-01-12:/2007_01_12-what_a_data_warehouse_can_never_do.html</id><summary type="html">&lt;p&gt;In one form, the question is &amp;quot;How do we handle
the [X] transaction in the warehouse?&amp;quot;  Another form of the question is &amp;quot;What do
we do when [Y] changes?&amp;quot;   The third form is less clear, but essentially the
same: &amp;quot;How do we maintain [Z] in the
warehouse?&amp;quot;&lt;/p&gt;
&lt;p&gt;All of these …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In one form, the question is &amp;quot;How do we handle
the [X] transaction in the warehouse?&amp;quot;  Another form of the question is &amp;quot;What do
we do when [Y] changes?&amp;quot;   The third form is less clear, but essentially the
same: &amp;quot;How do we maintain [Z] in the
warehouse?&amp;quot;&lt;/p&gt;
&lt;p&gt;All of these are questions
that superficially cover change management, but we're not really talking about
Kimball's &lt;strong&gt;Slowly Changing Dimension&lt;/strong&gt;  (SCD) design pattern.  It turns out,
we're talking about something more subtle and
confusing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;System of Record.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The real questions are
&lt;strong&gt;System of Record&lt;/strong&gt;  (SoR) questions.  In short, each
question is a version of &amp;quot;Where's the authoritative copy, and how do I keep it
current?&amp;quot;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The [X] transaction is, at least in
theory, part of a source application, a System of Record.  It is extracted from
SoR, transformed, and loaded into the warehouse.  The [X] transaction does not
change when the warehouse is implemented.  Unless, of course, there is no
SoR.&lt;/li&gt;
&lt;li&gt;The changes to [Y], similarly, should be
made in the SoR.  This is almost the same question as the &amp;quot;[X] transaction&amp;quot;
question, but it's asked about a piece of data, not a named business process.
The distinction reveals much about the processes which the warehouse must
support.&lt;/li&gt;
&lt;li&gt;The maintenance of [Z], clearly, should
be made in the SoR.  This is similar to the &amp;quot;changes to [Y]&amp;quot; question, but shows
a different point of view on what data is and why it
exists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We'll look at these questions
in a bit of depth.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transactions in the Warehouse.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When someone asks
about the &amp;quot;[X] transaction,&amp;quot; they're often summarizing a business process.  In
general, every business process is either informal, or formalized.  Informal
transactions are done manually using desktop tools: email, spreadsheet, word
processing, etc.  No piece of software captures, manages and enforces the
transaction.&lt;/p&gt;
&lt;p&gt;Formal transactions have
three general patterns for their SoR: one SoR, many SoR's and a badly-chosen
SoR.  When there's one SoR, life is good.  The transaction happens in some
system (SAP, Oracle, QuickBooks, Aptiva, etc.)  It propagates through the
organization through ordinary Enterprise Application Integration (EAI)
techniques.  It winds up in the warehouse through ordinary
Extract-Transform-Load (ETL)
processing.&lt;/p&gt;
&lt;p&gt;When there are multiple
SoR's, we have some challenges.  Sometimes, the relationship is
&lt;em&gt;horizontal&lt;/em&gt; :
two peer business units have separate sources for similar data.  One unit has
SAP, the other has Aptiva.  This means that there may be common data which must
be conformed into a warehouse dimension.  So far, so good.&lt;/p&gt;
&lt;p&gt;Sometimes the relationship between
SoR's is
&lt;em&gt;vertical&lt;/em&gt; :
the parent company uses SAP, the subsidiary uses Great Plains.   This means that
there may be contradictions between the views of the common data.  When data is
moved up from the subsidiary, it may be aggregated: business entities are
elided, and the data is difficult (or impossible) to
conform.&lt;/p&gt;
&lt;p&gt;Sometimes the relationship
between SoR's is
&lt;em&gt;psychotic&lt;/em&gt; .
This often leads to a badly-chosen SoR.  A single organization can have the same
data in two applications and neither can be trusted to be the SoR.  They may
have customer data in Siebel and JDE, and the data is different, and can only be
reconciled manually.  Sigh.  No amount of Data Warehouse ETL can sort this out.
The organization must pick something as the SoR, and revise their business
processes to reflect that.&lt;/p&gt;
&lt;p&gt;In summary,
there are no transactions in the warehouse.  Transactions happen in the SoR, and
the results of those transactions are applied to the warehouse.  You must pick
an SoR.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Change in the Warehouse.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Sometimes, there is no
System of Record.  There are two common cases: the data is maintained manually,
and the data is maintained through a cryptic transaction buried in the legacy
reporting application.  When data is maintained manually, we have a rather
difficult &lt;strong&gt;Master Data Management&lt;/strong&gt;  (MDM) issue because we don't have
an official SoR.  We're often in a bad position, here, because we're forced to
stop data warehouse development work to put a SoR in place.  This extra work can
be hard to justify; managers say &amp;quot;we never needed a system for that before, why
now?&amp;quot;&lt;/p&gt;
&lt;p&gt;The answer is simple, but
unpleasant.  &amp;quot;It never worked before, either.&amp;quot;  People put in data warehouses
because their legacy reporting tools are incorrect or inconsistent.  One root
cause of errors is lack of a public, well-understood truth because of manual or
informal changes.&lt;/p&gt;
&lt;p&gt;The cryptic
transaction is the worst thing to ferret out.  Let's say we have two
applications, B and C which each do parts of a business function.  Further, each
has it's little quirks, and we periodically must reconcile B and C's results
against each other.  How do we do this reconciliation when the two applications
are largely disjoint except where they have to be
reconciled?&lt;/p&gt;
&lt;p&gt;The usual solution is to
merge the data into a kind of data warehouse.  However, when there are
reconciliation problems, we hate to make a change to B or C, and re-run the
complete ETL cycle.  Instead we make the change directly in the warehouse.  Who
wants to duplicate this change in B or C?  No one, so we back-propagate the
change from the warehouse into the SoR's.  In effect, we've made the warehouse
the SoR.&lt;/p&gt;
&lt;p&gt;In summary, change in the
warehouse is limited to a historical snapshot of change in the SoR.  Change
happens in the SoR, and the results of the changes are applied to the warehouse.
You must pick an
SoR.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maintenance in the Warehouse.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The question of
maintaining data in the warehouse usually stems from a warehouse design which
involves something more complex than simple facts and dimensions.  Generally, a
bridge table (often for a hierarchy) becomes a source of confusion.  Most
business entities (dates, accounts, products, documents, etc.) are pretty clear
in the source applications.  The facts are usually
obvious.&lt;/p&gt;
&lt;p&gt;It's the reporting
relationships that get confusing.  Something like product family can be a very
difficult thing to handle.  Something like a bill of materials (BoM) or
Organization Hierarchy (OH) can be even more
complex.&lt;/p&gt;
&lt;p&gt;In the product family case,
the reporting is an organization fiction.  It doesn't tie back to anything
except how managers chunk information.  In this case, the reporting hierarchy is
entirely a feature of the warehouse itself.  This is the pure Master Data
Management problem, where business entities are grouped in the warehouse
exclusively for the user's
convenience.&lt;/p&gt;
&lt;p&gt;In the BoM or OH case,
however, the reporting hierarchy does have an independent existence.  In the
case of the BoM, it ties to engineering or product configuration.  In the case
of OH, it ties to some project structure or accounting structure.  However,
these hierarchical structures don't often exist in the same simple form that
they do in the warehouse bridge table.  And this leads to confusion on how we
maintain the bridge table.&lt;/p&gt;
&lt;p&gt;In summary,
maintenance in the warehouse is limited to loading a historical snapshot of the
relationships in the SoR.  Maintenance happens in the SoR, and the results of
the maintenance are applied to the warehouse.  You must depend on an
SoR.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bridge Tables Maintenance.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are several
varieties of Bridge Tables.  We'll address hierarchy, since it seems to lead to
the most confusion.  We'll touch on minidimension and outrigger tables, also,
since the same design pattern applies to
those.&lt;/p&gt;
&lt;p&gt;The essential worry about
hierarchies stems from the fact that a hierarchy bridge table can have many more
rows than the  dimension it bridges.  Generally, it's an
&lt;em&gt;n&lt;/em&gt; log(&lt;em&gt;n&lt;/em&gt; )
kind of multiplication, where
log(&lt;em&gt;n&lt;/em&gt; )
is an estimate of the depth of the
hierarchy.&lt;/p&gt;
&lt;p&gt;As a practical matter,
moving one child to another parent is a single row change in the original data.
However, the expansion in the bridge table means that
2*d*
rows will change, where
&lt;em&gt;d&lt;/em&gt;  is
the depth of the node in the hierarchy.  For some reason, this is
intimidating.&lt;/p&gt;
&lt;p&gt;There are two
solutions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Reload the entire bridge table with each
source change.  This is easy to implement but slow.  If you use SCD change
tracking, you'll have lots of nearly identical rows that are labeled with change
dates because they were associated with a source node change.&lt;/li&gt;
&lt;li&gt;Recompute just the changed parentage,
updating only those rows of the bridge table.  This is not significantly more
complex.  First, write a &amp;quot;find-all-parents&amp;quot; function, and apply this across
every element of the source data to populate the bridge initially.  Then, you
can use the &amp;quot;find-all-parents&amp;quot; function to compute just the relevant bridge
table changes when a source node changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A similar pattern is appropriate for
minidimensions and outriggers, which are based on subsets of a dimension.  The
lazy approach is to rebuild these each time the dimension changes.  A slightly
more efficient approach is to derive just the changed rows from the changes in
the dimension.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bottom Line.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Change doesn't happen in the
warehouse.  Change happens in the SoR.  The warehouse merely captures the effect
of that change.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>Refactoring and Unit Testing</title><link href="https://slott56.github.io/2006_10_11-refactoring_and_unit_testing.html" rel="alternate"></link><published>2006-10-11T00:19:00-04:00</published><updated>2006-10-11T00:19:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2006-10-11:/2006_10_11-refactoring_and_unit_testing.html</id><summary type="html">&lt;p&gt;I do a fair amount of manual refactoring.  I've
used WebSphere Studio (Eclipse) to do some automated refactoring, so I have some
experience in using IDE's which exploit Java's static
type-checking.&lt;/p&gt;
&lt;p&gt;However, the question of
type checking in a dynamic language is interesting.  I don't use a sophisticated
IDE for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I do a fair amount of manual refactoring.  I've
used WebSphere Studio (Eclipse) to do some automated refactoring, so I have some
experience in using IDE's which exploit Java's static
type-checking.&lt;/p&gt;
&lt;p&gt;However, the question of
type checking in a dynamic language is interesting.  I don't use a sophisticated
IDE for Python development.  So, I have limited experience using an IDE to do
refactoring in a dynamic
language.&lt;/p&gt;
&lt;p&gt;However, JB notes &amp;quot;I'm having
some heartburn about hierarchical
type&lt;/p&gt;
&lt;p&gt;systems as a way of determining 1)
conformability and 2) managing commitments
of&lt;/p&gt;
&lt;p&gt;semantically equivalent behavior. ...
Seems a constraining way&lt;/p&gt;
&lt;p&gt;to do it, to me, not
that I have a better alternative at the
moment.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Duck Typing.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since Python relies on
&lt;a class="reference external" href="http://en.wikipedia.org/wiki/Duck_typing"&gt;Duck
Typing&lt;/a&gt; , refactoring takes on an interesting new dimension.  We aren't
constrained to simply shuffle methods up and down the class hierarchy.  We are
now able to -- well -- put a method just about anywhere.&lt;/p&gt;
&lt;p&gt;Further, since Python doesn't have a
simple, single-inheritance model, the &amp;quot;hierarchical&amp;quot; type system doesn't
completely apply.&lt;/p&gt;
&lt;p&gt;For these reasons,
refactoring in Python is one potentially complex
problem.&lt;/p&gt;
&lt;p&gt;From what I understand of
Ruby, you can override a class method without creating a subclass, essentially
redefining a base class in some obscure way.  This gives me the willies because
it makes refactoring a problem without any sensible boundaries.  Maybe I'm
misunderstanding Ruby, and have this
wrong.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Essential Use Cases.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The principal refactoring
use case involves moving a common method up the inheritance hierarchy.  As a
practical matter, this does happen once in a
while.&lt;/p&gt;
&lt;p&gt;An additional text-book
refactoring use case arises when we're adding or removing whole methods in the
subclass hierarchy.&lt;/p&gt;
&lt;p&gt;The fringe use case
is a variation on the theme of
SubClass1.methodA
looking a lot like
Subclass2.methodA,
but they're not the same.  There are two interesting
cases.&lt;/p&gt;
&lt;p&gt;SC1.mA()
is a superset of
SC2.mA().  All
of SC2.mA()
gets refactored up, and SC1.mA()
overrides it to add
features.&lt;/p&gt;
&lt;p&gt;SC1.mA()
overlaps with
SC2.mA().  Some
common functionality has to get extracted, moved up the hierarchy;
SC1.mA() and
SC2.mA() are
rebuilt around this common
kernel.&lt;/p&gt;
&lt;p&gt;SC1.mA()
has no usable relationship with
SC2.mA().  What
now?  In some cases, a complete change in design may be called for.  The mere
presence of this situation is diagnostic of the designer having missed
something.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beyond the Fringe.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Outside the fringe of
ordinary refactoring are the &lt;strong&gt;New Design Pattern&lt;/strong&gt; ™ situations.  Mostly, these are
&lt;strong&gt;Strategy&lt;/strong&gt;
situations, where what looks -- initially -- like a variant method grows into a
different approach as we learn more about the
solution.&lt;/p&gt;
&lt;p&gt;Consider a pair of ordinary
Entity classes that look like different entities because of different behavior.
However, they have the same attributes, and almost identical methods.  The only
difference is one algorithm.  This can be done through inheritance, but
sometimes that variant algorithm is only the tip of the iceberg, and there is
more variability just below the
surface.&lt;/p&gt;
&lt;p&gt;At this point, we realize we
need a
&lt;strong&gt;Strategy&lt;/strong&gt;
hierarchy to contain the variant algorithms, not a hierarchy of ordinary
Entities.  How does refactoring work here, where we're moving the functionality
out of a class hierarchy into a different class hierarchy?  Is this even
refactoring, or is it the more general case of
redesign?&lt;/p&gt;
&lt;p&gt;It doesn't feel like
refactoring because  we aren't shuffling methods up and down the class
hierarchy.  In Python, the Duck Typing means we don't actually need a proper
hierarchy for the
&lt;strong&gt;Strategy&lt;/strong&gt;
class definitions.  Consequently, we're free to make significant structural
changes that I don't think an IDE can ever help
with.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Across the Spectrum.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One potential problem
with this is captured in the comment that &amp;quot;[the compiler] can’t help but
see your code as a pile of text&amp;quot;.  By extension, then, the IDE can't do anything
more than treat source as text, losing precious semantic information.&lt;/p&gt;
&lt;p&gt;However, when doing a fundamental
restructuring (from class methods to Strategy hierarchy), the source code
information available to the compiler (or the IDE) isn't of much value until
you've finished.  Nothing helps you when you're in the middle of this.  Until
the semantic information exists, no IDE can help you manage and maintain the
semantic information.&lt;/p&gt;
&lt;p&gt;There are parts
of design (and redesign) that are hard.  I think anyone would agree that the
earliest phases of noodling around about a problem are done without benefit of
an IDE or formal semantics.  When the design is merely conceptual, tools can't
help.&lt;/p&gt;
&lt;p&gt;I think refactoring includes a
very broad spectrum.  At one end, things are essentially mechanical; at the
other end things, are completely conceptual.  This isn't really a problem that
needs a solution; it doesn't need tools.  It's part of the game of moving from a
good idea to software.  Some parts of the good idea don't have formal semantics.
Eventually, when formal semantics exist, tools can be
applied.&lt;/p&gt;
&lt;p&gt;In the case of Python, I
suspect that IDE support for refactoring could only be feeble at best.  The
mechanical end of the spectrum is so easy that tools aren't required.   At the
conceptual end of the spectrum, tools don't help in the first
place.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Middle Ground.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One might argue that
simple, mechanical refactoring can be aided by the presence of static type
declarations.  However, my experience is that this covers only the most mundane
of the refactoring use cases.  In Python, we just move the method around.&lt;/p&gt;
&lt;p&gt;In Python, there's a double whammy:
checking types can't be done because the language traditionally lacked type
declarations.  Further -- and more important -- type checking doesn't need to be
done because the language is dynamic.  It can't be done, and even if it could,
it didn't matter anyway.&lt;/p&gt;
&lt;p&gt;This is overly
simplistic, however.  There is some type checking which can be done in Python.
The &lt;a class="reference external" href="http://epydoc.sourceforge.net/"&gt;epydoc&lt;/a&gt;  package does considerable analysis of
source as part of writing documentation.  It spots unused arguments, and can
spot certain kinds of obvious mismatches in number of arguments vs. parameters.&lt;/p&gt;
&lt;p&gt;When we look at JB's point on
committing to specific semantics, we see something even more profound.  It goes
way beyond what even Java is capable of checking or
automating.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Formal Specification.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;JB is asking for a
level beyond syntax, beyond type matching and off into &amp;quot;intent&amp;quot;.  JB appears to
be looking for formal assertions of preconditions and postconditions that he can
use to determine how to redesign methods to make them refactorable, and then how
to refactor the changed design.&lt;/p&gt;
&lt;p&gt;JB's
formality would be a nice thing to capture.  If every statement had a proper
precondition and postcondition, then we could prove almost anything about our
software except whether or not the loops actually terminated.  (That can't be
formally proven in a system with the same expressive power as software, it
requires more sophisticated logical
tools.)&lt;/p&gt;
&lt;p&gt;Since Java and Python have
added additional markers (annotations and decorators) JB's assertions could be
captured, to an extent.  You'd have to implement a simple &amp;quot;for all&amp;quot; and &amp;quot;there
exists&amp;quot; predicate, but Python has a nice reduce that can be paired with a lambda
that allows you to write a &amp;quot;for all&amp;quot;; from this you can built a &amp;quot;there exists&amp;quot;.&lt;/p&gt;
&lt;p&gt;I'm not sure how helpful formal
assertions would be.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pragmatic Refactoring.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When working in the
center of the refactoring use cases, IDE aids are helpful.  When working at the
fringe, they're just visual noise.  Indeed, when redesigning something, I have
to be sure not to look at any of the &amp;quot;helpful&amp;quot; messages from Eclipse because
it's checking for errors using obsolete type information.  When I've broken the
whole thing down into a workbench full of parts, the semantic checks aren't even
meaningful.  Once I get it put back together again, automated checking can be
handy to assure a complete job.&lt;/p&gt;
&lt;p&gt;In
Python, breaking the whole thing down as part of a redesign is so much simpler.
We don't have the artifice of &amp;quot;interface&amp;quot; to keep to a single inheritance model
with static type checking across multiple aspects of a class.  We just move the
methods around.  We have multiple inheritance, and we don't need formal
interface declarations.&lt;/p&gt;
&lt;p&gt;Indeed, it's
far, far easier to produce a working design in Python, and use that as a formal
specification for a Java program.  I can tweak and tinker, optimizing
performance and simplifying without the rigid formality of Java.  Adding proper
class hierarchies and turning multiple inheritance into single+interface
inheritance is typically a pretty easy transformation.  Since I knew I was
aiming at Java in the first place, I avoided Pythonisms that don't
translate.&lt;/p&gt;
&lt;p&gt;While it's true that we
don't need Java's formality in Python, much of that formality is helpful.  I
find it easier to work with a proper inheritance hierarchy, one that has
explicit Not Implemented exceptions to mark the place-holders.  I like to have a
tidy interface definition so that I can document the interface.  This additional
material makes refactoring slightly more complex, but could help an automated
tool do some useful method matching among
classes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Final Test.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Without appropriate unit
tests, refactoring is impossible.  Even in Java, with a swanky IDE that checks
everything, you still have potential problems which are uncheckable.  In
particular, a mis-named subclass method cannot be detected except by &amp;quot;near-miss&amp;quot;
fuzzy-matching rules that will almost always work and will have false-positives.
Only unit testing can locate this
situation.&lt;/p&gt;
&lt;p&gt;Unit testing absolutely is a
stand-in for things the compiler can't check.  You can portray the heavy use of
unit testing as a negative (&amp;quot;the compiler can't be trusted&amp;quot;) or as a pragmatic
approach to verifying the things you can't formally state.  All of the
assertions in the world won't find a spelling
mistake.&lt;/p&gt;
&lt;p&gt;Worse, your formal
declarations (post-condition assertions or type definitions) could just as
easily be wrong.  A tidy formal proof with a wrong piece of logic will derive an
incorrect program.  A misspelled class name may compile, but still fail a suite
of tests.&lt;/p&gt;
&lt;p&gt;Since the IDE can't register
intent very well, it isn't a complete solution.  In the case of redesign, it
isn't even very helpful.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>Python OODB (Revised)</title><link href="https://slott56.github.io/2006_06_20-python_oodb_revised.html" rel="alternate"></link><published>2006-06-20T10:35:00-04:00</published><updated>2006-06-20T10:35:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2006-06-20:/2006_06_20-python_oodb_revised.html</id><summary type="html">&lt;p&gt;Simple object persistence (i.e., serialization to
a file system) is what pickle, marshal and shelve
do.&lt;/p&gt;
&lt;p&gt;However, here's the next thing of
some interest OODB's.&lt;/p&gt;
&lt;p&gt;Zope's &lt;a class="reference external" href="http://www.zope.org/Wikis/ZODB/FrontPage"&gt;ZODB&lt;/a&gt; .  The original OODB for Python, the
backbone of Zope.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://buzhug.sourceforge.net/"&gt;buzhug&lt;/a&gt; :
&amp;quot;a fast, pure-Python database engine, using a syntax that Python programmers
should …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Simple object persistence (i.e., serialization to
a file system) is what pickle, marshal and shelve
do.&lt;/p&gt;
&lt;p&gt;However, here's the next thing of
some interest OODB's.&lt;/p&gt;
&lt;p&gt;Zope's &lt;a class="reference external" href="http://www.zope.org/Wikis/ZODB/FrontPage"&gt;ZODB&lt;/a&gt; .  The original OODB for Python, the
backbone of Zope.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://buzhug.sourceforge.net/"&gt;buzhug&lt;/a&gt; :
&amp;quot;a fast, pure-Python database engine, using a syntax that Python programmers
should find very intuitive.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://pypersyst.org/"&gt;PyPerSyst&lt;/a&gt;  &amp;quot;fast,
reliable, and flexible object persistence with a small footprint, suitable for
embedding in other Python
applications.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://couchdb.infogami.com/"&gt;CouchDB&lt;/a&gt;  &amp;quot;A
stand-alone document store, [which] most closely resembles the Lotus
Notes/Domino storage engine.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.objectivity.com/blogs/insider/expert_opinion/languages/python/"&gt;ObjectivityDB/Python&lt;/a&gt;  &amp;quot;a high performance and
robust Object-Oriented Database Management System [ODBMS].&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://divmod.org/trac/wiki/DivmodAxiom"&gt;Axiom&lt;/a&gt;  &amp;quot;Axiom is an object database, or
alternatively, an object-relational mapper.&amp;quot;  The pleasant thing is that you
don't really care which.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.mems-exchange.org/software/durus/"&gt;Durus&lt;/a&gt;  &amp;quot;a persistent object system for
applications written in the Python programming
language.&amp;quot;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.itamarst.org/software/cog/"&gt;http://www.itamarst.org/software/cog/&lt;/a&gt;
(not &lt;a class="reference external" href="http://wiki.cogkit.org/index.php/Main_Page"&gt;CoG&lt;/a&gt; , the Commodity Grid) is the Checkpointed
Object Graph, which provides &amp;quot;semi-transparent persistence for large sets of
interrelated Python objects.&amp;quot;&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>Doctest beyond Python</title><link href="https://slott56.github.io/2006_04_17-doctest_beyond_python.html" rel="alternate"></link><published>2006-04-17T14:53:00-04:00</published><updated>2006-04-17T14:53:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2006-04-17:/2006_04_17-doctest_beyond_python.html</id><summary type="html">&lt;p&gt;This is something that elevates Doctest into the
realm of Pattern.  Perhaps even above
that.&lt;/p&gt;
&lt;p&gt;The idea is so elegant: the
document is the test, and the test procedure is the
document.&lt;/p&gt;
&lt;p&gt;There's a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Don't_repeat_yourself"&gt;DRY&lt;/a&gt;  clarity to the whole thing that is rather
exciting.  It is an elegant application of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is something that elevates Doctest into the
realm of Pattern.  Perhaps even above
that.&lt;/p&gt;
&lt;p&gt;The idea is so elegant: the
document is the test, and the test procedure is the
document.&lt;/p&gt;
&lt;p&gt;There's a &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Don't_repeat_yourself"&gt;DRY&lt;/a&gt;  clarity to the whole thing that is rather
exciting.  It is an elegant application of basic &lt;a class="reference external" href="http://en.wikipedia.org/wiki/Literate_programming"&gt;Literate Programming&lt;/a&gt;
principles.&lt;/p&gt;
&lt;p&gt;The best part is that it is
difficult to do in other languages.  Ruby and Perl have the necessary
interactive execution modes.  But in Java, it would be a nightmare to define all
the required overheads to have a standardized exercise framework that paralleled
the Python interactive execution
mode.&lt;/p&gt;
&lt;p&gt;This makes the &amp;quot;Doctest&amp;quot; pattern
a key value proposition for any new language or environment.  If you can
implement the Doctest pattern, you have something that creates value by binding
testing and documentation into one tidy package.  If you can't implement the
Doctest pattern, perhaps you should rethink your implementation because you
can't easily compete against Python.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>Python Object-Relational Mapping (Revised)</title><link href="https://slott56.github.io/2006_04_13-python_object_relational_mapping_revised.html" rel="alternate"></link><published>2006-04-13T02:37:00-04:00</published><updated>2006-04-13T02:37:00-04:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2006-04-13:/2006_04_13-python_object_relational_mapping_revised.html</id><summary type="html">&lt;p&gt;Ian Bicking: A Blog &lt;a class="reference external" href="http://blog.ianbicking.org/"&gt;http://blog.ianbicking.org/&lt;/a&gt;,
provided some info on Py3K and Python Introspection &lt;a class="reference external" href="http://blog.ianbicking.org/introspecting-expressions-in-py3k.html"&gt;http://blog.ianbicking.org/introspecting-expressions-in-py3k.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For
me, the interesting part was his summary of Object-Relational Mapping.  Mr.
Bicking identifies two broad approaches: lambda introspection and operator
overloading.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lamba Introspection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://projects.amor.org/dejavu"&gt;Dejavu&lt;/a&gt;
It primarily uses …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ian Bicking: A Blog &lt;a class="reference external" href="http://blog.ianbicking.org/"&gt;http://blog.ianbicking.org/&lt;/a&gt;,
provided some info on Py3K and Python Introspection &lt;a class="reference external" href="http://blog.ianbicking.org/introspecting-expressions-in-py3k.html"&gt;http://blog.ianbicking.org/introspecting-expressions-in-py3k.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For
me, the interesting part was his summary of Object-Relational Mapping.  Mr.
Bicking identifies two broad approaches: lambda introspection and operator
overloading.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lamba Introspection&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://projects.amor.org/dejavu"&gt;Dejavu&lt;/a&gt;
It primarily uses a generic &lt;a class="reference external" href="http://www.martinfowler.com/eaaCatalog/dataMapper.html"&gt;Data Mapper&lt;/a&gt;  architecture.  It is more of an OODB
backed by a relational store.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://subway.python-hosting.com/wiki/SQLComp"&gt;SQLComp&lt;/a&gt;  The make_query method examines a lambda
containing a list comprehension to create SQL.  This is only queries, and isn't
a complete ORM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Operator Overloading&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://sqlobject.org/"&gt;SQLObject&lt;/a&gt;  This is
a very complete ORM, cast in the some mold as
Django.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://sqlalchemy.org/"&gt;SQLAlchemy&lt;/a&gt; This provides a Pythonic definition
of SQL metadata and a mapping from the SQL metadata to Python class definitions.
This is a very, very rich approach, allowing you to straddle the SQL and Object
worlds explicitly.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://pyorq.sourceforge.net/"&gt;PyORQ&lt;/a&gt;   This
is an older ORM with a few data types but a very &amp;quot;naked&amp;quot; use of overloaded
operators to perform queries.  Unlike the lambda overloading, the class provides
operators that are set operations for
queries.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Non-Introspective Approaches&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.djangoproject.com/"&gt;Django&lt;/a&gt; , for example, encodes attributes and
operators as keyword parameters to methods.  It doesn't look inside the Python
code, but parses the keywords.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://skunkweb.sourceforge.net/"&gt;PyDO2&lt;/a&gt;
encodes the query explicitly, using functions that mirror SQL operators or
tuples that contain string names for the
functions.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.qlime.org/"&gt;QLime&lt;/a&gt;   is an ORM with functional notation,
similar to PyDO2.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.tux4web.de/computer/software/orm/"&gt;ORM&lt;/a&gt;  (the Object-Relational Membrane) mostly
captures SQL metadata in Python.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://projects.almad.net/dbclass"&gt;DBClass&lt;/a&gt;
is focused on an easy way to hack around with SQL queries (to get data from
procedures and so on).&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://divmod.org/trac/wiki/DivmodAxiom"&gt;Axiom&lt;/a&gt;  is an object database, or alternatively,
an object-relational mapper.  It depends on &lt;a class="reference external" href="http://divmod.org/trac/wiki/DivmodEpsilon"&gt;Epsilon&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;The
Python wiki page on &lt;a class="reference external" href="http://wiki.python.org/moin/HigherLevelDatabaseProgramming"&gt;Higher Level Database Programming&lt;/a&gt;   has
additional notes and products that are high
level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Garden-Variety Relational Access&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All of these modules
provide the standard &lt;a class="reference external" href="http://www.python.org/dev/peps/pep-0249/"&gt;DB-API&lt;/a&gt;  (PEP 249) interface to a SQL database.&lt;/p&gt;
&lt;p&gt;The most visible access layer product
is &lt;a class="reference external" href="http://www.egenix.com/files/python/eGenix-mx-Extensions.html"&gt;mx.ODBC&lt;/a&gt;  for bare ODBC connectivity.  This has
the advantage of wide portability, and the disadvantage of the narrow ODBC
interface.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://pdo.neurokode.com/"&gt;PDO&lt;/a&gt;  wraps a
variety of other access methods into a single, combined package.  I'm not
precisely sure why it adds another interface layer, but it appears to simply do
away with Cursor objects.  However, it does provide a nice list of DB-API 2.0
modules for direct SQL access.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://sourceforge.net/projects/mysql-python"&gt;MySQLdb&lt;/a&gt;  for
MySQL&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://initd.org/tracker/pysqlite"&gt;PySQLite&lt;/a&gt;  and &lt;a class="reference external" href="http://www.rogerbinns.com/apsw.html"&gt;APSW&lt;/a&gt;
are for the ultra-lightweight SQLite
RDBMS.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="http://python.projects.postgresql.org/"&gt;PostgresPy&lt;/a&gt;   project will address many PostgreSQL
topics.  &lt;a class="reference external" href="http://www.pygresql.org/"&gt;PyGreSql&lt;/a&gt;  (aka pgdb), &lt;a class="reference external" href="http://www.initd.org/projects/psycopg1"&gt;psycopg&lt;/a&gt; , &lt;a class="reference external" href="http://www.zope.org/Members/tm/PoPy"&gt;PoPy&lt;/a&gt; ,
&lt;a class="reference external" href="http://barryp.org/software/bpgsql"&gt;bpgsql&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://kinterbasdb.sourceforge.net/"&gt;kinterbasdb&lt;/a&gt;  Firebird and Borland's
Interbase&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://sourceforge.net/projects/pydb2/"&gt;pyDB2&lt;/a&gt;
DB/2&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.cxtools.net/default.aspx?nav=cxorlb%22%20target=%22NewWindow"&gt;cx_Oracle&lt;/a&gt;
Oracle&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://adodbapi.sourceforge.net/"&gt;adodbapi&lt;/a&gt;
Python access to the MS Windows ADO
interface&lt;/p&gt;
&lt;p&gt;The Python wiki page on &lt;a class="reference external" href="http://wiki.python.org/moin/DatabaseInterfaces"&gt;Database Interfaces&lt;/a&gt;  also has a list of these
product-specific access
modules.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recommendations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Rule
1.  Do development with SQLite.  Why? Eschew Features.  Focus on RDBMS for
relational store, focus on Python for processing.  Stored Procedures and
Triggers are a product-specific mine-field.  Once the model passes unit tests,
move to another RDBMS that supports concurrent
users.&lt;/p&gt;
&lt;p&gt;Rule 2.  For OLTP, use an
OR-Mapping and stay away from naked SQL.  However (and this is a big however)
you will likely be supporting ad-hoc reporting through SQL-based report writers.
There are two extemes.  At one end is Deja-Vu, which may be too far from the
underlying SQL.   The other end begins with SQLAlchemy, which may expose too
much SQL; ORM and DBClass may be too light on object
features.&lt;/p&gt;
&lt;p&gt;Rule 3.  For OLAP, you have
two kinds of applications.  Some parts (like dimension conformance) can use an
OR-Mapping because they are OLAP-like.  For some loading, aggregation and
extraction, use direct SQL drivers for the chosen product.  For the large-volume
fact-oriented loads, use the vendor-supplied bulk loader.  Portability is not
your concern.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="database"></category></entry><entry><title>Agile Testing Goodies from PyCon 2006</title><link href="https://slott56.github.io/2006_02_27-agile_testing_goodies_from_pycon_2006.html" rel="alternate"></link><published>2006-02-27T11:26:00-05:00</published><updated>2006-02-27T11:26:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2006-02-27:/2006_02_27-agile_testing_goodies_from_pycon_2006.html</id><summary type="html">&lt;p&gt;A number of testing frameworks were used.  The
Agile Testing tutorial provides a path through the toolsets, showing what you
can do, and how you should do it.&lt;/p&gt;
&lt;p&gt;Unit
Testing:  [&lt;a class="reference external" href="http://somethingaboutorange.com/mrl/projects/nose/"&gt;Nose&lt;/a&gt; ], &amp;lt;{filename}/blog/2005/11/2005_11_09-compare_and_contrast_round_3_revised.rst&amp;gt;&lt;/p&gt;
&lt;p&gt;Acceptance
Testing:  [&lt;a class="reference external" href="http://fitnesse.org/FrontPage"&gt;FitNesse&lt;/a&gt; ]&lt;/p&gt;
&lt;p&gt;Regression
Testing:  [&lt;a class="reference external" href="http://texttest.carmen.se/index.html"&gt;TextTest&lt;/a&gt; ]&lt;/p&gt;
&lt;p&gt;Functional
Testing:  [&lt;a class="reference external" href="http://www.idyll.org/~t/www-tools/twill/"&gt;twill&lt;/a&gt; ].  A thorough analysis is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A number of testing frameworks were used.  The
Agile Testing tutorial provides a path through the toolsets, showing what you
can do, and how you should do it.&lt;/p&gt;
&lt;p&gt;Unit
Testing:  [&lt;a class="reference external" href="http://somethingaboutorange.com/mrl/projects/nose/"&gt;Nose&lt;/a&gt; ], &amp;lt;{filename}/blog/2005/11/2005_11_09-compare_and_contrast_round_3_revised.rst&amp;gt;&lt;/p&gt;
&lt;p&gt;Acceptance
Testing:  [&lt;a class="reference external" href="http://fitnesse.org/FrontPage"&gt;FitNesse&lt;/a&gt; ]&lt;/p&gt;
&lt;p&gt;Regression
Testing:  [&lt;a class="reference external" href="http://texttest.carmen.se/index.html"&gt;TextTest&lt;/a&gt; ]&lt;/p&gt;
&lt;p&gt;Functional
Testing:  [&lt;a class="reference external" href="http://www.idyll.org/~t/www-tools/twill/"&gt;twill&lt;/a&gt; ].  A thorough analysis is at &lt;a class="reference external" href="http://www.advogato.org/article/874.html"&gt;http://www.advogato.org/article/874.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ajax
Interaction Testing:  [&lt;a class="reference external" href="http://www.openqa.org/selenium/"&gt;Selenium&lt;/a&gt; ] and PAMIE &lt;a class="reference external" href="http://pamie.sourceforge.net/"&gt;http://pamie.sourceforge.net/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Literate
Testing:  [&lt;a class="reference external" href="http://www.python.org/doc/lib/module-doctest.html"&gt;doctest&lt;/a&gt; ] and epydoc &lt;a class="reference external" href="http://epydoc.sourceforge.net/"&gt;http://epydoc.sourceforge.net/&lt;/a&gt;.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>testresources</title><link href="https://slott56.github.io/2005_12_26-testresources.html" rel="alternate"></link><published>2005-12-26T13:57:00-05:00</published><updated>2005-12-26T13:57:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2005-12-26:/2005_12_26-testresources.html</id><summary type="html">&lt;p&gt;testresources' &lt;a class="reference external" href="http://www.robertcollins.net/unittest/testresources/"&gt;http://www.robertcollins.net/unittest/testresources/&lt;/a&gt;
purpose appears to be to manage the resources used by a test
suite.&lt;/p&gt;
&lt;p&gt;Adding this resource management
context extends the &lt;strong&gt;Test Suite&lt;/strong&gt;  to optimize tests around the resources.
This can reshuffle the TestCases to minimize SetUp's.  This can be useful in
contexts where …&lt;/p&gt;</summary><content type="html">&lt;p&gt;testresources' &lt;a class="reference external" href="http://www.robertcollins.net/unittest/testresources/"&gt;http://www.robertcollins.net/unittest/testresources/&lt;/a&gt;
purpose appears to be to manage the resources used by a test
suite.&lt;/p&gt;
&lt;p&gt;Adding this resource management
context extends the &lt;strong&gt;Test Suite&lt;/strong&gt;  to optimize tests around the resources.
This can reshuffle the TestCases to minimize SetUp's.  This can be useful in
contexts where the
&lt;strong&gt;Fixture&lt;/strong&gt;
includes
&lt;strong&gt;Singletons&lt;/strong&gt;
or expensive resources.&lt;/p&gt;
&lt;p&gt;While
interesting, this package bends one of the common definitions of Unit Testing.
If there is a complex resource dependency, the
&lt;strong&gt;Fixture&lt;/strong&gt;
being tested isn't really isolated.  This pushes beyond isolated unit testing
with
&lt;strong&gt;Mock&lt;/strong&gt;
objects into integration testing with real objects and real
interfaces.&lt;/p&gt;
&lt;p&gt;One can make the case that
&amp;quot;unit&amp;quot; is intentionally vague; the Beck definitions refer to a
&lt;strong&gt;Fixture&lt;/strong&gt;
as the design pattern.  This could be a class, module or package, depending on
your willingness to abstract.  I agree that &amp;quot;unit&amp;quot; does not necessarily mean
class.  However, I do think that &amp;quot;unit&amp;quot; means isolated from other
components.&lt;/p&gt;
&lt;p&gt;testresources seems
specifically designed for integration test, not unit test.  I think it is
miscategorized, and belongs to an unidentified species of products: integration
testing frameworks.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>SubUnit</title><link href="https://slott56.github.io/2005_12_20-subunit.html" rel="alternate"></link><published>2005-12-20T18:36:00-05:00</published><updated>2005-12-20T18:36:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2005-12-20:/2005_12_20-subunit.html</id><summary type="html">&lt;p&gt;SubUnit's &lt;a class="reference external" href="http://www.robertcollins.net/unittest/subunit/"&gt;http://www.robertcollins.net/unittest/subunit/&lt;/a&gt; purpose appears to be to manage testing
via subprocesses.&lt;/p&gt;
&lt;p&gt;Consequently, it can
run external tests not in Python, it can fork a subprocess to manage the Fixture
in an isolated process.&lt;/p&gt;
&lt;p&gt;Adding this
subprocess execution context extends the
&lt;strong&gt;Test Runner&lt;/strong&gt;  implementation of the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;SubUnit's &lt;a class="reference external" href="http://www.robertcollins.net/unittest/subunit/"&gt;http://www.robertcollins.net/unittest/subunit/&lt;/a&gt; purpose appears to be to manage testing
via subprocesses.&lt;/p&gt;
&lt;p&gt;Consequently, it can
run external tests not in Python, it can fork a subprocess to manage the Fixture
in an isolated process.&lt;/p&gt;
&lt;p&gt;Adding this
subprocess execution context extends the
&lt;strong&gt;Test Runner&lt;/strong&gt;  implementation of the built-in
unittest
module.  This can be useful in contexts where the Fixture includes
&lt;strong&gt;Singletons&lt;/strong&gt;
or connection pools or other per-process design features.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>Twisted Trial</title><link href="https://slott56.github.io/2005_12_15-twisted_trial.html" rel="alternate"></link><published>2005-12-15T17:10:00-05:00</published><updated>2005-12-15T17:10:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2005-12-15:/2005_12_15-twisted_trial.html</id><summary type="html">&lt;p&gt;Trial is not really a stand-alone unit test
framework.  It is an extension to unittest focused on the testing needs for the
Twisted framework.&lt;/p&gt;
&lt;p&gt;The Trial how-to
&lt;a class="reference external" href="http://twistedmatrix.com/projects/core/documentation/howto/testing.html"&gt;http://twistedmatrix.com/projects/core/documentation/howto/testing.html&lt;/a&gt; has some information.  More valuable,
perhaps are the API documents &lt;a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.trial.html"&gt;http://twistedmatrix.com/documents …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Trial is not really a stand-alone unit test
framework.  It is an extension to unittest focused on the testing needs for the
Twisted framework.&lt;/p&gt;
&lt;p&gt;The Trial how-to
&lt;a class="reference external" href="http://twistedmatrix.com/projects/core/documentation/howto/testing.html"&gt;http://twistedmatrix.com/projects/core/documentation/howto/testing.html&lt;/a&gt; has some information.  More valuable,
perhaps are the API documents &lt;a class="reference external" href="http://twistedmatrix.com/documents/current/api/twisted.trial.html"&gt;http://twistedmatrix.com/documents/current/api/twisted.trial.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Because of the asynchronous nature of
Twisted, the Fixture pattern has a rather complex relationship with the Twisted
Reactor.  Also, since Twisted is a framework, not a single application,
components are optional, and the TestSuite pattern is implemented with
considerably more flexibility.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>More Frameworks! (rev. 3)</title><link href="https://slott56.github.io/2005_12_13-more_frameworks_rev_3.html" rel="alternate"></link><published>2005-12-13T18:40:00-05:00</published><updated>2005-12-13T18:40:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2005-12-13:/2005_12_13-more_frameworks_rev_3.html</id><summary type="html">&lt;p&gt;A wiki page on Python testing tools &lt;a class="reference external" href="http://pycheesecake.org/wiki/PythonTestingToolsTaxonomy"&gt;http://pycheesecake.org/wiki/PythonTestingToolsTaxonomy&lt;/a&gt; identifies a number of additional unit
testing tools.  The wiki page provides a handy summary.  I'll examine these in
light of the Beck Unit Test design patterns to provide a little more detail on
what they really do …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A wiki page on Python testing tools &lt;a class="reference external" href="http://pycheesecake.org/wiki/PythonTestingToolsTaxonomy"&gt;http://pycheesecake.org/wiki/PythonTestingToolsTaxonomy&lt;/a&gt; identifies a number of additional unit
testing tools.  The wiki page provides a handy summary.  I'll examine these in
light of the Beck Unit Test design patterns to provide a little more detail on
what they really do.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Twisted Trial&lt;/li&gt;
&lt;li&gt;SubUnit&lt;/li&gt;
&lt;li&gt;Testresources&lt;/li&gt;
&lt;li&gt;PyUnitPerf&lt;/li&gt;
&lt;li&gt;PeckCheck&lt;/li&gt;
&lt;li&gt;PythonMock&lt;/li&gt;
&lt;li&gt;pMock&lt;/li&gt;
&lt;/ul&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>Compare and Contrast (round 3, revised)</title><link href="https://slott56.github.io/2005_11_09-compare_and_contrast_round_3_revised.html" rel="alternate"></link><published>2005-11-09T19:38:00-05:00</published><updated>2005-11-09T19:38:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2005-11-09:/2005_11_09-compare_and_contrast_round_3_revised.html</id><summary type="html">&lt;p&gt;The object-oriented unit testing framework began
as Smalltalk's Beck Test framework &lt;a class="reference external" href="http://www.xprogramming.com/testfram.htm"&gt;http://www.xprogramming.com/testfram.htm&lt;/a&gt;.  It evolved to the JUnit &lt;a class="reference external" href="http://www.junit.org/index.htm"&gt;http://www.junit.org/index.htm&lt;/a&gt;&amp;gt;`_ `  &amp;lt;&lt;a class="reference external" href="http://www.junit.org/index.htm%22%20target=%22NewWindow"&gt;http://www.junit.org/index.htm%22%20target=%22NewWindow&lt;/a&gt;
framework for Java.  Beck defined four repeated patterns of unit testing
software …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The object-oriented unit testing framework began
as Smalltalk's Beck Test framework &lt;a class="reference external" href="http://www.xprogramming.com/testfram.htm"&gt;http://www.xprogramming.com/testfram.htm&lt;/a&gt;.  It evolved to the JUnit &lt;a class="reference external" href="http://www.junit.org/index.htm"&gt;http://www.junit.org/index.htm&lt;/a&gt;&amp;gt;`_ `  &amp;lt;&lt;a class="reference external" href="http://www.junit.org/index.htm%22%20target=%22NewWindow"&gt;http://www.junit.org/index.htm%22%20target=%22NewWindow&lt;/a&gt;
framework for Java.  Beck defined four repeated patterns of unit testing
software, covered in a previous posting &amp;lt;{filename}/blog/2005/11/2005_11_05-compare_and_contrast_round_1.rst&amp;gt;.&lt;/p&gt;
&lt;p&gt;Nose, TestOOB, test.py
(and TestGears) are significant revisions to the
&lt;strong&gt;Test Suite&lt;/strong&gt;  and
&lt;strong&gt;Test Runner&lt;/strong&gt;  parts of the unit test patterns.
Additionally, these tools make efforts to implement the
&lt;strong&gt;Diagnostics&lt;/strong&gt;
pattern, also.&lt;/p&gt;
&lt;p&gt;The
&lt;strong&gt;Fixture&lt;/strong&gt;
is implied by the context in which the tests are discovered.  Nose can locate
package, module and function tests; it uses TestCase class definitions, also.
TestOOB and test.py sit more squarely on unittest, with the resulting focus on
module-level testing.&lt;/p&gt;
&lt;p&gt;In TestOOB and
test.py, the
&lt;strong&gt;TestCase&lt;/strong&gt;
class plus a flexible regular expressions or glob expression defines the test
cases.  test.py looks for packages of tests, using the path name of the package,
as well as the module name.  In Nose, the
&lt;strong&gt;TestCase&lt;/strong&gt;
class can be used for compatibility, but this is not required; Nose will match a
regular expression to locate tests.&lt;/p&gt;
&lt;p&gt;The
&lt;strong&gt;Results Check&lt;/strong&gt;  in nose can be done via the existing
assert statement.  Nose, pleasantly handles the &amp;quot;test which throws an exception&amp;quot;
case: a test function that exits normally is a &amp;quot;pass&amp;quot;.  An
AssertionError
exception is a test failure; any other exception is an error.  Since TestOOB and
test.py sit on
unittest, they
depend on the complex set of assert methods, and the fail
method.&lt;/p&gt;
&lt;p&gt;The
&lt;strong&gt;Test Suite&lt;/strong&gt;  is implied by the collection of TestCase
instances with the expected name forms in TestOOB.  In Nose and test.py, it is
the collection of modules, functions and methods with names that have the
expected forms.  Both cases make powerful use of Python introspection to track
down the tests.&lt;/p&gt;
&lt;p&gt;The
&lt;strong&gt;Test Runner&lt;/strong&gt;  in nose can be a stand-alone
nosetests
program, or you can import nose;
nose.main().  In the case of TestOOB, we have a
testoob
program, or we can import testoob;
testoob.main().    Nose has an interesting
integration with Python distutils/setuptools.  It adds a new &amp;quot;test&amp;quot; verb to
setup.py.  The test.py main program has a large number of options to fine-tune
which tests are run&lt;/p&gt;
&lt;p&gt;Nose supports the
&lt;strong&gt;Diagnostics&lt;/strong&gt;
with output capture and a simple flag for producing additional details.
TestOOB, in certain environments, will produce color output; it produces an XML
test report as well as HTML test reports.  TestOOB can launch the Python
debugger as well as log failing assertions in detail.  test.py can run
pychecker, do tracing and refcount checking as part of the
diagnostics.&lt;/p&gt;
&lt;p&gt;TestOOB has some
additional features for repeating and controlling the timing of the tests.
While this is not sufficient to prove that an application lacks the kind of race
condition that makes it behave poorly; it can help to provide some confidence
for load testing.  Similarly, test.py includes features for looping tests to
look for memory leaks and race conditions.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>Compare and Contrast (round 2)</title><link href="https://slott56.github.io/2005_11_07-compare_and_contrast_round_2.html" rel="alternate"></link><published>2005-11-07T17:50:00-05:00</published><updated>2005-11-07T17:50:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2005-11-07:/2005_11_07-compare_and_contrast_round_2.html</id><summary type="html">&lt;p&gt;The object-oriented unit testing framework began
as Smalltalk's Beck Test framework &lt;a class="reference external" href="http://www.xprogramming.com/testfram.htm%22%20target=%22NewWindow"&gt;http://www.xprogramming.com/testfram.htm%22%20target=%22NewWindow&lt;/a&gt;.
It evolved to the JUnit &lt;a class="reference external" href="http://www.junit.org/index.htm%22%20target=%22NewWindow"&gt;http://www.junit.org/index.htm%22%20target=%22NewWindow&lt;/a&gt;
framework for Java.  Beck defined four repeated patterns of unit testing
software, covered in a previous …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The object-oriented unit testing framework began
as Smalltalk's Beck Test framework &lt;a class="reference external" href="http://www.xprogramming.com/testfram.htm%22%20target=%22NewWindow"&gt;http://www.xprogramming.com/testfram.htm%22%20target=%22NewWindow&lt;/a&gt;.
It evolved to the JUnit &lt;a class="reference external" href="http://www.junit.org/index.htm%22%20target=%22NewWindow"&gt;http://www.junit.org/index.htm%22%20target=%22NewWindow&lt;/a&gt;
framework for Java.  Beck defined four repeated patterns of unit testing
software, covered in a previous posting &amp;lt;{filename}/blog/2005/11/2005_11_05-compare_and_contrast_round_1.rst&amp;gt;.&lt;/p&gt;
&lt;p&gt;An
additional pattern that py.test introduces is the
&lt;strong&gt;Diagnostics&lt;/strong&gt;
pattern.  This is a useful traceback or cached output.  To make it useful, it is
presented only for failing tests, and elides repetition in the event of
recursions that lead to stack
overflows.&lt;/p&gt;
&lt;p&gt;py.test seems to deliver
most of the Beck-defined features.&lt;/p&gt;
&lt;p&gt;The Fixture is created by offering a
number of setup/teardown functions, either at the module level (for a module or
class) or within a class.&lt;/p&gt;
&lt;p&gt;The Test
Case is a module, class or function with an appropriate name.  Either
&lt;tt class="docutils literal"&gt;test_&lt;/tt&gt; or
&lt;tt class="docutils literal"&gt;Test_&lt;/tt&gt; as a
prefix is sufficient to define a test
case.&lt;/p&gt;
&lt;p&gt;The Results Check uses ordinary
asserts and a special
py.test.raises
function to cover all the bases.  Personally, I prefer the JUnit approach to
catching the expected exception and calling the
fail() method
for everything else.&lt;/p&gt;
&lt;p&gt;The Suite is
developed by implication through Python's powerful introspection: everything
that looks like a test -- at the package, module and class level -- is a
candidate.  A regular expression can pick names, plus other global conditions
can be examined to further refine the test protocols.&lt;/p&gt;
&lt;p&gt;The Runner is a stand-alone
py.test program
that locates the tests, executes them and produces a log.  Further, it produces
Diagnostics focused on the failing tests.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>Compare and Contrast (round 1)</title><link href="https://slott56.github.io/2005_11_05-compare_and_contrast_round_1.html" rel="alternate"></link><published>2005-11-05T20:21:00-05:00</published><updated>2005-11-05T20:21:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2005-11-05:/2005_11_05-compare_and_contrast_round_1.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Some Basis for Comparison&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The object-oriented unit
testing framework began as Smalltalk's Beck Test framework &lt;a class="reference external" href="http://www.xprogramming.com/testfram.htm"&gt;http://www.xprogramming.com/testfram.htm&lt;/a&gt;.  It evolved to the JUnit &lt;a class="reference external" href="http://www.junit.org/index.htm"&gt;http://www.junit.org/index.htm&lt;/a&gt;
framework for Java.  Beck defined four repeated patterns of unit testing
software:&lt;/p&gt;
&lt;p&gt;The
&lt;strong&gt;Fixture&lt;/strong&gt; .
The thing we are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Some Basis for Comparison&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The object-oriented unit
testing framework began as Smalltalk's Beck Test framework &lt;a class="reference external" href="http://www.xprogramming.com/testfram.htm"&gt;http://www.xprogramming.com/testfram.htm&lt;/a&gt;.  It evolved to the JUnit &lt;a class="reference external" href="http://www.junit.org/index.htm"&gt;http://www.junit.org/index.htm&lt;/a&gt;
framework for Java.  Beck defined four repeated patterns of unit testing
software:&lt;/p&gt;
&lt;p&gt;The
&lt;strong&gt;Fixture&lt;/strong&gt; .
The thing we are testing; a class or possibly a set of instances of a given
class, or possibly something even larger.  If we are testing more than one class
at a time, we aren't really &amp;quot;unit&amp;quot; testing.  So the fixture often includes stubs
for missing classes.&lt;/p&gt;
&lt;p&gt;The
&lt;strong&gt;Test Case&lt;/strong&gt; .  A predictable reaction of the fixture.
This should either work or fail.  It can, of course also raise one of those
egregious, unchecked-for errors that indicate fairly serious problems in a
preliminary piece of software.  Or, it may indicate something that was badly
damaged during maintenance and is now raising errors instead of simply failing
the regression test suite.&lt;/p&gt;
&lt;p&gt;The
&lt;strong&gt;Results Check&lt;/strong&gt; .  A specific assertion about the
fixture's results.&lt;/p&gt;
&lt;p&gt;The
&lt;strong&gt;Test Suite&lt;/strong&gt; . A collection of
TestCases.&lt;/p&gt;
&lt;p&gt;JUnit and unittest add a
&lt;strong&gt;Test Runner&lt;/strong&gt;  pattern, also.  This the top-level
component that uses a Test Suite to create test results by executing each Test
Case, assuring that each Results Check worked.  The Test Runner can also assure
that any Fixture Setup and Teardown is done
correctly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Legacy Frameworks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;unittest delivers all
the Beck-defined features.  It should, it is the indirect descendant of the
original framework.  Having JUnit as an ancestor, however, leads to some clunky
non-Pythonic features.  In particular, Python features that Java lacks are
ignored, including modules and free-standing
functions.&lt;/p&gt;
&lt;p&gt;doctest has an odd fit with
the Beck framework.  The fixture isn't well defined; since doctest has a
module-centric view, a shallow copy of the module globals are given to each
test, making the module globals the fixture.  Each Case and Results Check is
encoded in a docstring, usually by a cut and paste from an interactive testing
session.  The test suite is implied by the module
structure.&lt;/p&gt;
&lt;p&gt;unittest isn't terribly
Pythonic.  Doctest is module-focused, not class focused, and doesn't treat the
notion of fixture very well.&lt;/p&gt;
&lt;p&gt;IMO,
module-based testing is a more useful level of unit testing.  Individual
classes, while important, rarely make sense in a vacuum.  All of the test
harness and stub classes required to test just one class seems like too much
unproductive work.  When the architecture changes, I may have to change a class
definition as well as the test harness classes that stand in for this class in
the unit testing framework.&lt;/p&gt;
&lt;p&gt;Next Up,
py.test, nose and testgears.  Later, TestOOB and Sancho.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry><entry><title>Python Unit Testing Frameworks (v3)</title><link href="https://slott56.github.io/2005_11_02-python_unit_testing_frameworks_v3.html" rel="alternate"></link><published>2005-11-02T00:12:00-05:00</published><updated>2005-11-02T00:12:00-05:00</updated><author><name>S.Lott</name></author><id>tag:slott56.github.io,2005-11-02:/2005_11_02-python_unit_testing_frameworks_v3.html</id><summary type="html">&lt;p&gt;Ned Batchelder : Blog [&lt;a class="reference external" href="http://www.nedbatchelder.com/blog/index.html"&gt;http://www.nedbatchelder.com/blog/index.html&lt;/a&gt; ] identifies no less than 6 unit testing
frameworks for Python [&lt;a class="reference external" href="http://www.nedbatchelder.com/blog/200510.html#e20051025T070731"&gt;http://www.nedbatchelder.com/blog/200510.html#e20051025T070731&lt;/a&gt; ] and [&lt;a class="reference external" href="http://www.nedbatchelder.com/blog/200411.html#e20041120T185622"&gt;http://www.nedbatchelder.com/blog/200411.html#e20041120T185622&lt;/a&gt; ].&lt;/p&gt;
&lt;p&gt;TestGears
[&lt;a class="reference external" href="http://www.turbogears.com/testgears/"&gt;http://www.turbogears.com/testgears/&lt;/a&gt; ] is part of the TurboGears web
uber-framework …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ned Batchelder : Blog [&lt;a class="reference external" href="http://www.nedbatchelder.com/blog/index.html"&gt;http://www.nedbatchelder.com/blog/index.html&lt;/a&gt; ] identifies no less than 6 unit testing
frameworks for Python [&lt;a class="reference external" href="http://www.nedbatchelder.com/blog/200510.html#e20051025T070731"&gt;http://www.nedbatchelder.com/blog/200510.html#e20051025T070731&lt;/a&gt; ] and [&lt;a class="reference external" href="http://www.nedbatchelder.com/blog/200411.html#e20041120T185622"&gt;http://www.nedbatchelder.com/blog/200411.html#e20041120T185622&lt;/a&gt; ].&lt;/p&gt;
&lt;p&gt;TestGears
[&lt;a class="reference external" href="http://www.turbogears.com/testgears/"&gt;http://www.turbogears.com/testgears/&lt;/a&gt; ] is part of the TurboGears web
uber-framework. It provides automatic discovery of test functions, simplifies
suite development, and makes it easy to run tests zero configuration.  Kevin
Dangoor [&lt;a class="reference external" href="http://www.blueskyonmars.com/"&gt;http://www.blueskyonmars.com/&lt;/a&gt; ] says he will deprecate this in favor of
Nose.  David Warnock [&lt;a class="reference external" href="http://42.blogs.warnock.me.uk/2005/10/turbogears_cont.html"&gt;http://42.blogs.warnock.me.uk/2005/10/turbogears_cont.html&lt;/a&gt; ] says something
similar.&lt;/p&gt;
&lt;p&gt;TestOOB [&lt;a class="reference external" href="http://testoob.sourceforge.net/"&gt;http://testoob.sourceforge.net/&lt;/a&gt; ]
(Testing Out Of [the] Box) provides for new styles of output (HTML and color
terminal), debugger launching, verbose asserts, parallel execution, and
command-line utility testing&lt;/p&gt;
&lt;p&gt;nose [&lt;a class="reference external" href="http://somethingaboutorange.com/mrl/projects/nose/"&gt;http://somethingaboutorange.com/mrl/projects/nose/&lt;/a&gt; ] provides an alternate test discovery and
execution engine for unittest&lt;/p&gt;
&lt;p&gt;unittest [&lt;a class="reference external" href="http://docs.python.org/lib/module-unittest.html"&gt;http://docs.python.org/lib/module-unittest.html&lt;/a&gt; ], formerly known as PyUnit [&lt;a class="reference external" href="http://pyunit.sourceforge.net/"&gt;http://pyunit.sourceforge.net/&lt;/a&gt; ]
(Thanks for the heads up, Tony [&lt;a class="reference external" href="http://www.haloscan.com/comments/slott/E20051105152154/#29209"&gt;http://www.haloscan.com/comments/slott/E20051105152154/#29209&lt;/a&gt; ])&lt;/p&gt;
&lt;p&gt;doctest
[&lt;a class="reference external" href="http://docs.python.org/lib/module-doctest.html"&gt;http://docs.python.org/lib/module-doctest.html&lt;/a&gt; ]&lt;/p&gt;
&lt;p&gt;py.test [&lt;a class="reference external" href="http://codespeak.net/py/current/doc/test.html"&gt;http://codespeak.net/py/current/doc/test.html&lt;/a&gt; ]&lt;/p&gt;
&lt;p&gt;Michal
Watkins [&lt;a class="reference external" href="http://mikewatkins.net/"&gt;http://mikewatkins.net/&lt;/a&gt; ] adds  Sancho, a unit testing framework
[&lt;a class="reference external" href="http://www.mems-exchange.org/software/sancho/"&gt;http://www.mems-exchange.org/software/sancho/&lt;/a&gt; ].&lt;/p&gt;
&lt;p&gt;Also,
ZOPE has test.py [&lt;a class="reference external" href="http://zopewiki.org/HowToRunZopeUnitTests"&gt;http://zopewiki.org/HowToRunZopeUnitTests&lt;/a&gt; ].  There is a derivative product, also, the
SchoolTool Test Runner [&lt;a class="reference external" href="http://svn.nuxeo.org/trac/pub/file/CalCore/trunk/test.py"&gt;http://svn.nuxeo.org/trac/pub/file/CalCore/trunk/test.py&lt;/a&gt; ].&lt;/p&gt;
&lt;p&gt;Jeremy
Hylton's blog has some notes [&lt;a class="reference external" href="http://www.python.org/~jeremy/weblog/031014.html"&gt;http://www.python.org/~jeremy/weblog/031014.html&lt;/a&gt; ] on unit testing, describing
test.py.&lt;/p&gt;
&lt;p&gt;Ian Bicking also has a list of
complaints about the basic unittest interface [&lt;a class="reference external" href="http://blog.colorstudy.com/ianb/weblog/2003/10/10.html#P11"&gt;http://blog.colorstudy.com/ianb/weblog/2003/10/10.html#P11&lt;/a&gt; ], many of which are answered by the
add-ons.&lt;/p&gt;
</content><category term="Python"></category><category term="#python"></category><category term="unit testing"></category></entry></feed>